{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import collections\n",
    "import operator\n",
    "import itertools\n",
    "import pickle\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import simpleclock\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset, iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.path.abspath(''), \"data_cine_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(tokenize = \"spacy\",\n",
    "                            tokenizer_language=\"fr_core_news_sm\",\n",
    "                            include_lengths=True)\n",
    "LABEL = torchtext.data.LabelField(dtype=torch.float, use_vocab=False, preprocessing=lambda x: float(x) / 5)\n",
    "# labels are linearly rescaled to a 0-1 range\n",
    "# todo: test if preprocessing data before isn't faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchtext.data.TabularDataset(path=data_path,\n",
    "                                        format=\"CSV\",\n",
    "                                        fields={\"critique\": (\"input\", TEXT), \"note\": (\"target\", LABEL)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = dataset.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_valid = data_train.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: 36057 examples.\n",
      "validation data: 15453 examples.\n",
      "test data: 22076 examples.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"training data: {len(data_train)} examples.\n",
    "validation data: {len(data_valid)} examples.\n",
    "test data: {len(data_test)} examples.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = torchtext.vocab.Vectors(\"cc.fr.300.vec\", os.path.join(os.path.expanduser(\"~\"), \"Downloads\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_MAX_SIZE = 50000\n",
    "TEXT.build_vocab(data_train, max_size=VOCAB_MAX_SIZE, vectors=vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_train, iter_valid, iter_test = \\\n",
    "    torchtext.data.BucketIterator.splits(datasets=(data_train, data_valid, data_test),\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         device=DEVICE,\n",
    "                                         sort_within_batch=True,\n",
    "                                         sort_key=lambda example: len(example.input),\n",
    "                                         sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, n_vocab, embedding_dim, hidden_dim, output_dim, dropout, bidirectional,\n",
    "                 n_layers, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        num_dir = 2 if bidirectional else 1\n",
    "        self.embedding = torch.nn.Embedding(n_vocab, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = torch.nn.LSTM(embedding_dim,\n",
    "                                 hidden_dim,\n",
    "                                 bidirectional=bidirectional,\n",
    "                                 num_layers=n_layers)\n",
    "        self.fc = torch.nn.Linear(hidden_dim * num_dir, output_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_lengths):\n",
    "        input, lengths = input_lengths\n",
    "        torch.nn.utils.rnn.pack_padded_sequence(input, lengths)\n",
    "        embedded = self.embedding(input)  # ((sent_len, batch), emb_dim)\n",
    "        packed_output, (hidden, cell) = self.rnn(embedded)  # hidden: (num_layers * num_directions,\n",
    "                                                            #          batch, hidden_size * num_directions)\n",
    "        hidden = (torch.cat([hidden[-2, :, :], hidden[-1, :, :]], dim=1)\n",
    "                  if self.bidirectional else hidden).squeeze(0)  # (batch, hidden_size * num_directions)\n",
    "        return self.sigmoid(self.fc(self.dropout(hidden)))  # (batch, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "DEFAULT_PARAMS = {\n",
    "    \"n_vocab\": len(TEXT.vocab),\n",
    "    \"embedding_dim\": 300,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"output_dim\": 1,\n",
    "    \"dropout\": 0.5,\n",
    "    \"bidirectional\": True,\n",
    "    \"n_layers\": 1,\n",
    "    \"pad_idx\": PAD_IDX,\n",
    "}\n",
    "\n",
    "def default_model(**kwargs):\n",
    "    _d = {}\n",
    "    _d.update(DEFAULT_PARAMS)\n",
    "    _d.update(kwargs)\n",
    "    return RNN(**_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_init(model, criterion, device=DEVICE, learn_embedding_param=True):\n",
    "    model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(model.embedding_dim)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(model.embedding_dim)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if name == \"embedding.weight\":\n",
    "            param.requires_grad = learn_embedding_param\n",
    "    \n",
    "    print(\"The model has {:,} trainable parameters\"\n",
    "         .format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    \n",
    "    return model, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_pred(output):\n",
    "    return (output * 10).round() / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, y):\n",
    "    correct = (preds == y).float()  # convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch.input).squeeze(1)\n",
    "        loss = criterion(output, batch.target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = accuracy(output_to_pred(output), batch.target * 5)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "      \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            output = model(batch.input).squeeze(1)\n",
    "            loss = criterion(output, batch.target)\n",
    "            acc = accuracy(output_to_pred(output), batch.target * 5)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainInfo:\n",
    "    def __init__(self, valid={}, train={}):\n",
    "        self.valid = collections.defaultdict(lambda: [])\n",
    "        self.valid.update(valid)\n",
    "        self.train = collections.defaultdict(lambda: [])\n",
    "        self.train.update(train)\n",
    "    \n",
    "    def save(self, path):\n",
    "        packed = {\n",
    "            \"valid\": dict(self.valid),\n",
    "            \"train\": dict(self.train),\n",
    "        }\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(packed, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            packed = pickle.load(f)\n",
    "            return cls(valid=packed[\"valid\"],\n",
    "                       train=packed[\"train\"])\n",
    "    \n",
    "    @staticmethod\n",
    "    def _dict_to_repr(d):\n",
    "        return dict(map(lambda k_v: (k_v[0], f\"{len(k_v[1])} elements\"), d.items()))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return pprint.pformat({\"valid\": self._dict_to_repr(self.valid),\n",
    "                     \"train\": self._dict_to_repr(self.train),})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_training(model, name, iter_train, iter_valid, optimizer, criterion, fun_train,\n",
    "                fun_eval, n_epochs=100, train_info=None):\n",
    "\n",
    "    clock = simpleclock.Clock.started()\n",
    "    torch.cuda.empty_cache()\n",
    "    train_info = train_info if train_info is not None else TrainInfo()\n",
    "    best_valid_loss = min(train_info.valid[\"loss\"]) if train_info.valid[\"loss\"] else float(\"inf\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        clock.elapsed_since_start.call()  # meh\n",
    "\n",
    "        train_loss, train_acc = fun_train(model, iter_train, optimizer, criterion)\n",
    "        valid_loss, valid_acc = fun_eval(model, iter_valid, criterion)\n",
    "        \n",
    "        is_best = valid_loss < best_valid_loss\n",
    "        print(\"Epoch: {e:<3}. T, V acc: {train:.1f}%, {valid:.1f}%. Took {t:.2}s.\"\n",
    "             .format(e=epoch + 1,\n",
    "                     train=100 * train_acc,\n",
    "                     valid=100 * valid_acc,\n",
    "                     t=clock.elapsed_since_last_call())\n",
    "             + (\" (+)\" if is_best else \"\")\n",
    "             )\n",
    "        \n",
    "        train_info.train[\"loss\"].append(train_loss)\n",
    "        train_info.valid[\"loss\"].append(valid_loss)\n",
    "\n",
    "        if is_best:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f\"{name}.pt\")\n",
    "\n",
    "    clock.elapsed_since_start.print(f\"Trained {name}, {n_epochs} epochs, for\")\n",
    "    return train_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainSet:\n",
    "    def __init__(self, model, name, iter_train, iter_valid,\n",
    "                 fun_optimizer, fun_criterion, fun_train, fun_eval,\n",
    "                 device=DEVICE, n_epochs=100):\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.iter_train = iter_train\n",
    "        self.iter_valid = iter_valid\n",
    "        self.fun_optimizer = fun_optimizer\n",
    "        self.fun_criterion = fun_criterion\n",
    "        self.fun_train = fun_train\n",
    "        self.fun_eval = fun_eval\n",
    "        self.n_epochs = n_epochs\n",
    "        self.device = device\n",
    "        \n",
    "        self.optimizer = None\n",
    "        self.criterion = None\n",
    "        \n",
    "    def init(self, learn_embedding_param=True):\n",
    "        self.model, self.criterion = pseudo_init(self.model, self.fun_criterion(), self.device,\n",
    "                                                 learn_embedding_param=learn_embedding_param)\n",
    "        self.optimizer = self.fun_optimizer(self.model.parameters())\n",
    "    \n",
    "    def do_training(self):\n",
    "        if self.optimizer is None or self.criterion is None:\n",
    "            raise Exception(\"It looks like an init is needed: optimizer or criterion is None\")\n",
    "        return do_training(model=self.model,\n",
    "                           name=self.name,\n",
    "                           iter_train=self.iter_train,\n",
    "                           iter_valid=self.iter_valid,\n",
    "                           optimizer=self.optimizer,\n",
    "                           criterion=self.criterion,\n",
    "                           fun_train=self.fun_train,\n",
    "                           fun_eval=self.fun_eval,\n",
    "                           n_epochs=self.n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving/loading utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab_embedding(path, vocab, embedding):\n",
    "    with open(path, \"w\") as f:\n",
    "        for word, vector in tqdm.tqdm(zip(vocab.itos, embedding)):\n",
    "            \n",
    "            # skip words with unicode symbols\n",
    "            if len(word) != len(word.encode()):\n",
    "                continue\n",
    "            \n",
    "            # 'words' like \" \" or \"\\n\" fail to be loaded\n",
    "            if word.strip() == \"\":\n",
    "                continue\n",
    "\n",
    "            f.write(f\"{word} {' '.join(str(e) for e in vector.tolist())}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_context(model, field, path_model, path_vocab, cache_embeddings=\"cache_embeddings\", device=DEVICE):\n",
    "    \n",
    "    _vectors = torchtext.vocab.Vectors(path_vocab, cache_embeddings)  # voir unk_init\n",
    "    field.build_vocab(data_train, max_size=VOCAB_MAX_SIZE, vectors=_vectors)\n",
    "    \n",
    "    model.load_state_dict(torch.load(path_model))\n",
    "    \n",
    "    model.embedding.weight.data.copy_(field.vocab.vectors)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    iter_train, iter_valid, iter_test = \\\n",
    "        torchtext.data.BucketIterator.splits(datasets=(data_train, data_valid, data_test),\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             device=DEVICE,\n",
    "                                             sort_within_batch=True,\n",
    "                                             sort_key=lambda example: len(example.input),\n",
    "                                             sort=False)\n",
    "    \n",
    "    return model, (iter_train, iter_valid, iter_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sets = []\n",
    "N_EPOCHS = 10000\n",
    "\n",
    "for hidden_dim, n_layers in itertools.product([256], [3]):\n",
    "    train_sets.append(TrainSet(\n",
    "        model=default_model(hidden_dim=hidden_dim, n_layers=n_layers),\n",
    "        name=f\"rnn_hidden-{hidden_dim}_nlayers-{n_layers}_nepochs-{N_EPOCHS}\",\n",
    "        iter_train=iter_train,\n",
    "        iter_valid=iter_valid,\n",
    "        fun_optimizer=torch.optim.Adam,\n",
    "        fun_criterion=torch.nn.MSELoss,\n",
    "        fun_train=train,\n",
    "        fun_eval=evaluate,\n",
    "        n_epochs=N_EPOCHS\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 19,297,817 trainable parameters\n",
      "Epoch: 1  . T, V acc: 20.1%, 27.1%. Took: 11.32s\n",
      "Epoch: 2  . T, V acc: 29.9%, 27.5%. Took: 11.22s\n",
      "Epoch: 3  . T, V acc: 32.0%, 24.5%. Took: 11.05s\n",
      "Epoch: 4  . T, V acc: 37.4%, 23.2%. Took: 11.07s\n",
      "Epoch: 5  . T, V acc: 44.3%, 26.2%. Took: 11.22s\n",
      "Epoch: 6  . T, V acc: 50.7%, 24.6%. Took: 11.49s\n",
      "Epoch: 7  . T, V acc: 59.3%, 25.7%. Took: 11.24s\n",
      "Epoch: 8  . T, V acc: 64.7%, 24.8%. Took: 11.29s\n",
      "Epoch: 9  . T, V acc: 69.6%, 25.2%. Took: 11.12s\n",
      "Epoch: 10 . T, V acc: 72.0%, 25.8%. Took: 11.03s\n",
      "Epoch: 11 . T, V acc: 77.1%, 26.6%. Took: 11.02s\n",
      "Epoch: 12 . T, V acc: 80.7%, 26.8%. Took: 11.23s\n",
      "Epoch: 13 . T, V acc: 83.8%, 27.7%. Took: 11.20s\n",
      "Epoch: 14 . T, V acc: 85.2%, 27.8%. Took: 11.18s\n",
      "Epoch: 15 . T, V acc: 85.5%, 28.0%. Took: 11.14s\n",
      "Epoch: 16 . T, V acc: 87.1%, 28.9%. Took: 11.11s\n",
      "Epoch: 17 . T, V acc: 85.8%, 29.6%. Took: 11.16s\n",
      "Epoch: 18 . T, V acc: 86.1%, 28.0%. Took: 11.17s\n",
      "Epoch: 19 . T, V acc: 88.7%, 29.9%. Took: 11.16s\n",
      "Epoch: 20 . T, V acc: 91.0%, 30.3%. Took: 11.16s\n",
      "Epoch: 21 . T, V acc: 92.4%, 32.4%. Took: 11.15s\n",
      "Epoch: 22 . T, V acc: 94.2%, 33.1%. Took: 11.17s\n",
      "Epoch: 23 . T, V acc: 95.0%, 33.9%. Took: 11.17s\n",
      "Epoch: 24 . T, V acc: 91.3%, 32.8%. Took: 11.15s\n",
      "Epoch: 25 . T, V acc: 92.1%, 34.3%. Took: 11.16s\n",
      "Epoch: 26 . T, V acc: 94.5%, 35.0%. Took: 11.14s\n",
      "Epoch: 27 . T, V acc: 92.8%, 34.1%. Took: 11.15s\n",
      "Epoch: 28 . T, V acc: 93.8%, 34.9%. Took: 11.20s\n",
      "Epoch: 29 . T, V acc: 95.5%, 34.7%. Took: 11.17s\n",
      "Epoch: 30 . T, V acc: 96.4%, 36.6%. Took: 11.13s\n",
      "Epoch: 31 . T, V acc: 96.9%, 37.0%. Took: 11.15s\n",
      "Epoch: 32 . T, V acc: 96.3%, 37.0%. Took: 11.18s\n",
      "Epoch: 33 . T, V acc: 97.5%, 37.8%. Took: 11.20s\n",
      "Epoch: 34 . T, V acc: 97.5%, 37.6%. Took: 11.32s\n",
      "Epoch: 35 . T, V acc: 97.5%, 37.0%. Took: 11.30s\n",
      "Epoch: 36 . T, V acc: 97.0%, 38.3%. Took: 11.32s\n",
      "Epoch: 37 . T, V acc: 97.1%, 39.1%. Took: 11.37s\n",
      "Epoch: 38 . T, V acc: 97.1%, 38.1%. Took: 11.29s\n",
      "Epoch: 39 . T, V acc: 97.3%, 39.4%. Took: 11.67s\n",
      "Epoch: 40 . T, V acc: 97.0%, 37.9%. Took: 11.40s\n",
      "Epoch: 41 . T, V acc: 97.3%, 38.8%. Took: 11.33s\n",
      "Epoch: 42 . T, V acc: 97.6%, 39.1%. Took: 11.45s\n",
      "Epoch: 43 . T, V acc: 97.4%, 38.3%. Took: 11.37s\n",
      "Epoch: 44 . T, V acc: 97.9%, 39.8%. Took: 11.33s\n",
      "Epoch: 45 . T, V acc: 98.2%, 39.5%. Took: 11.35s\n",
      "Epoch: 46 . T, V acc: 97.2%, 38.8%. Took: 11.34s\n",
      "Epoch: 47 . T, V acc: 97.5%, 39.3%. Took: 11.74s\n",
      "Epoch: 48 . T, V acc: 98.0%, 39.2%. Took: 11.34s\n",
      "Epoch: 49 . T, V acc: 98.4%, 38.9%. Took: 11.32s\n",
      "Epoch: 50 . T, V acc: 98.9%, 40.0%. Took: 11.38s\n",
      "Epoch: 51 . T, V acc: 99.0%, 40.6%. Took: 11.42s\n",
      "Epoch: 52 . T, V acc: 97.4%, 35.3%. Took: 11.34s\n",
      "Epoch: 53 . T, V acc: 95.1%, 38.0%. Took: 11.35s\n",
      "Epoch: 54 . T, V acc: 97.7%, 38.8%. Took: 11.36s\n",
      "Epoch: 55 . T, V acc: 98.3%, 40.5%. Took: 11.40s\n",
      "Epoch: 56 . T, V acc: 98.6%, 41.1%. Took: 11.38s\n",
      "Epoch: 57 . T, V acc: 99.0%, 40.4%. Took: 11.45s\n",
      "Epoch: 58 . T, V acc: 99.0%, 39.5%. Took: 11.38s\n",
      "Epoch: 59 . T, V acc: 99.1%, 41.0%. Took: 11.31s\n",
      "Epoch: 60 . T, V acc: 99.2%, 40.8%. Took: 11.50s\n",
      "Epoch: 61 . T, V acc: 99.2%, 40.6%. Took: 11.38s\n",
      "Epoch: 62 . T, V acc: 98.8%, 38.5%. Took: 11.38s\n",
      "Epoch: 63 . T, V acc: 98.3%, 40.4%. Took: 11.39s\n",
      "Epoch: 64 . T, V acc: 98.6%, 39.9%. Took: 11.36s\n",
      "Epoch: 65 . T, V acc: 98.8%, 40.3%. Took: 11.43s\n",
      "Epoch: 66 . T, V acc: 98.9%, 40.4%. Took: 11.41s\n",
      "Epoch: 67 . T, V acc: 98.7%, 40.4%. Took: 11.36s\n",
      "Epoch: 68 . T, V acc: 98.7%, 41.2%. Took: 11.38s\n",
      "Epoch: 69 . T, V acc: 98.9%, 40.4%. Took: 11.44s\n",
      "Epoch: 70 . T, V acc: 99.1%, 40.8%. Took: 11.44s\n",
      "Epoch: 71 . T, V acc: 99.2%, 41.9%. Took: 11.39s\n",
      "Epoch: 72 . T, V acc: 99.2%, 41.2%. Took: 11.36s\n",
      "Epoch: 73 . T, V acc: 99.3%, 40.4%. Took: 11.37s\n",
      "Epoch: 74 . T, V acc: 99.3%, 41.1%. Took: 11.38s\n",
      "Epoch: 75 . T, V acc: 99.0%, 39.9%. Took: 11.36s\n",
      "Epoch: 76 . T, V acc: 98.9%, 40.8%. Took: 11.39s\n",
      "Epoch: 77 . T, V acc: 97.7%, 37.8%. Took: 11.32s\n",
      "Epoch: 78 . T, V acc: 97.1%, 39.0%. Took: 11.38s\n",
      "Epoch: 79 . T, V acc: 98.6%, 41.0%. Took: 11.41s\n",
      "Epoch: 80 . T, V acc: 99.0%, 40.5%. Took: 11.45s\n",
      "Epoch: 81 . T, V acc: 99.2%, 40.9%. Took: 11.29s\n",
      "Epoch: 82 . T, V acc: 99.4%, 41.6%. Took: 11.30s\n",
      "Epoch: 83 . T, V acc: 99.6%, 41.0%. Took: 11.30s\n",
      "Epoch: 84 . T, V acc: 99.5%, 41.8%. Took: 11.27s\n",
      "Epoch: 85 . T, V acc: 99.5%, 41.1%. Took: 11.28s\n",
      "Epoch: 86 . T, V acc: 99.4%, 40.3%. Took: 11.29s\n",
      "Epoch: 87 . T, V acc: 99.2%, 40.1%. Took: 11.32s\n",
      "Epoch: 88 . T, V acc: 99.0%, 40.5%. Took: 11.30s\n",
      "Epoch: 89 . T, V acc: 99.1%, 39.9%. Took: 11.33s\n",
      "Epoch: 90 . T, V acc: 99.2%, 41.0%. Took: 11.27s\n",
      "Epoch: 91 . T, V acc: 99.4%, 40.6%. Took: 11.29s\n",
      "Epoch: 92 . T, V acc: 99.4%, 41.1%. Took: 11.26s\n",
      "Epoch: 93 . T, V acc: 99.5%, 40.9%. Took: 11.29s\n",
      "Epoch: 94 . T, V acc: 99.4%, 40.8%. Took: 11.30s\n",
      "Epoch: 95 . T, V acc: 99.4%, 40.3%. Took: 11.24s\n",
      "Epoch: 96 . T, V acc: 99.2%, 40.7%. Took: 11.29s\n",
      "Epoch: 97 . T, V acc: 99.2%, 40.7%. Took: 11.31s\n",
      "Epoch: 98 . T, V acc: 99.2%, 40.1%. Took: 11.32s\n",
      "Epoch: 99 . T, V acc: 98.8%, 40.7%. Took: 11.33s\n",
      "Epoch: 100. T, V acc: 99.1%, 39.8%. Took: 11.24s\n",
      "Epoch: 101. T, V acc: 99.4%, 40.9%. Took: 11.27s\n",
      "Epoch: 102. T, V acc: 99.6%, 40.2%. Took: 11.31s\n",
      "Epoch: 103. T, V acc: 99.4%, 41.2%. Took: 11.32s\n",
      "Epoch: 104. T, V acc: 99.4%, 41.0%. Took: 11.27s\n",
      "Epoch: 105. T, V acc: 99.4%, 40.7%. Took: 11.26s\n",
      "Epoch: 106. T, V acc: 99.5%, 40.7%. Took: 11.12s\n",
      "Epoch: 107. T, V acc: 99.3%, 41.1%. Took: 11.06s\n",
      "Epoch: 108. T, V acc: 99.1%, 40.2%. Took: 11.08s\n",
      "Epoch: 109. T, V acc: 99.0%, 41.0%. Took: 11.10s\n",
      "Epoch: 110. T, V acc: 99.3%, 40.6%. Took: 11.33s\n",
      "Epoch: 111. T, V acc: 99.4%, 40.7%. Took: 11.31s\n",
      "Epoch: 112. T, V acc: 99.5%, 40.9%. Took: 11.26s\n",
      "Epoch: 113. T, V acc: 99.6%, 41.7%. Took: 11.33s\n",
      "Epoch: 114. T, V acc: 99.6%, 41.4%. Took: 11.34s\n",
      "Epoch: 115. T, V acc: 99.6%, 40.9%. Took: 11.34s\n",
      "Epoch: 116. T, V acc: 99.6%, 40.7%. Took: 11.35s\n",
      "Epoch: 117. T, V acc: 99.5%, 40.0%. Took: 11.31s\n",
      "Epoch: 118. T, V acc: 99.4%, 40.6%. Took: 11.32s\n",
      "Epoch: 119. T, V acc: 99.6%, 40.9%. Took: 11.32s\n",
      "Epoch: 120. T, V acc: 99.6%, 40.9%. Took: 11.34s\n",
      "Epoch: 121. T, V acc: 99.5%, 40.8%. Took: 11.34s\n",
      "Epoch: 122. T, V acc: 99.5%, 41.0%. Took: 11.31s\n",
      "Epoch: 123. T, V acc: 99.7%, 41.4%. Took: 11.11s\n",
      "Epoch: 124. T, V acc: 99.7%, 42.0%. Took: 11.13s\n",
      "Epoch: 125. T, V acc: 99.6%, 40.4%. Took: 11.11s\n",
      "Epoch: 126. T, V acc: 99.3%, 41.2%. Took: 11.16s\n",
      "Epoch: 127. T, V acc: 99.3%, 40.5%. Took: 11.21s\n",
      "Epoch: 128. T, V acc: 99.1%, 40.9%. Took: 11.31s\n",
      "Epoch: 129. T, V acc: 99.4%, 41.0%. Took: 11.35s\n",
      "Epoch: 130. T, V acc: 99.5%, 41.0%. Took: 11.32s\n",
      "Epoch: 131. T, V acc: 99.5%, 41.7%. Took: 11.31s\n",
      "Epoch: 132. T, V acc: 99.3%, 41.1%. Took: 11.31s\n",
      "Epoch: 133. T, V acc: 99.5%, 40.9%. Took: 11.29s\n",
      "Epoch: 134. T, V acc: 99.6%, 40.9%. Took: 11.31s\n",
      "Epoch: 135. T, V acc: 99.6%, 41.0%. Took: 11.31s\n",
      "Epoch: 136. T, V acc: 99.6%, 40.1%. Took: 11.27s\n",
      "Epoch: 137. T, V acc: 99.5%, 40.3%. Took: 11.24s\n",
      "Epoch: 138. T, V acc: 99.7%, 41.0%. Took: 11.30s\n",
      "Epoch: 139. T, V acc: 99.6%, 40.4%. Took: 11.27s\n",
      "Epoch: 140. T, V acc: 99.5%, 40.5%. Took: 11.30s\n",
      "Epoch: 141. T, V acc: 99.6%, 41.1%. Took: 11.31s\n",
      "Epoch: 142. T, V acc: 99.7%, 41.1%. Took: 11.31s\n",
      "Epoch: 143. T, V acc: 99.7%, 40.8%. Took: 11.28s\n",
      "Epoch: 144. T, V acc: 99.7%, 41.2%. Took: 11.33s\n",
      "Epoch: 145. T, V acc: 99.7%, 41.0%. Took: 11.33s\n",
      "Epoch: 146. T, V acc: 99.8%, 41.3%. Took: 11.28s\n",
      "Epoch: 147. T, V acc: 99.7%, 40.6%. Took: 11.30s\n",
      "Epoch: 148. T, V acc: 99.7%, 40.7%. Took: 11.33s\n",
      "Epoch: 149. T, V acc: 99.6%, 41.1%. Took: 11.30s\n",
      "Epoch: 150. T, V acc: 99.5%, 40.7%. Took: 11.30s\n",
      "Epoch: 151. T, V acc: 99.4%, 40.3%. Took: 11.29s\n",
      "Epoch: 152. T, V acc: 99.6%, 40.6%. Took: 11.36s\n",
      "Epoch: 153. T, V acc: 99.6%, 41.1%. Took: 11.35s\n",
      "Epoch: 154. T, V acc: 99.8%, 41.0%. Took: 11.28s\n",
      "Epoch: 155. T, V acc: 99.8%, 40.2%. Took: 11.26s\n",
      "Epoch: 156. T, V acc: 99.7%, 41.5%. Took: 11.31s\n",
      "Epoch: 157. T, V acc: 99.5%, 41.2%. Took: 11.33s\n",
      "Epoch: 158. T, V acc: 99.6%, 40.8%. Took: 11.30s\n",
      "Epoch: 159. T, V acc: 99.5%, 41.4%. Took: 11.29s\n",
      "Epoch: 160. T, V acc: 99.5%, 41.5%. Took: 11.29s\n",
      "Epoch: 161. T, V acc: 99.2%, 39.7%. Took: 11.26s\n",
      "Epoch: 162. T, V acc: 99.3%, 40.8%. Took: 11.29s\n",
      "Epoch: 163. T, V acc: 99.5%, 41.0%. Took: 11.29s\n",
      "Epoch: 164. T, V acc: 99.6%, 41.2%. Took: 11.27s\n",
      "Epoch: 165. T, V acc: 99.7%, 41.6%. Took: 11.29s\n",
      "Epoch: 166. T, V acc: 99.8%, 41.0%. Took: 11.23s\n",
      "Epoch: 167. T, V acc: 99.8%, 41.3%. Took: 11.29s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 168. T, V acc: 99.7%, 41.4%. Took: 11.31s\n",
      "Epoch: 169. T, V acc: 99.8%, 41.5%. Took: 11.37s\n",
      "Epoch: 170. T, V acc: 99.6%, 39.2%. Took: 11.29s\n",
      "Epoch: 171. T, V acc: 99.6%, 40.5%. Took: 11.21s\n",
      "Epoch: 172. T, V acc: 99.1%, 40.6%. Took: 11.28s\n",
      "Epoch: 173. T, V acc: 99.4%, 40.8%. Took: 11.26s\n",
      "Epoch: 174. T, V acc: 99.6%, 40.2%. Took: 11.27s\n",
      "Epoch: 175. T, V acc: 99.7%, 40.8%. Took: 11.29s\n",
      "Epoch: 176. T, V acc: 99.7%, 40.6%. Took: 11.29s\n",
      "Epoch: 177. T, V acc: 99.7%, 40.7%. Took: 11.31s\n",
      "Epoch: 178. T, V acc: 99.8%, 40.6%. Took: 11.28s\n",
      "Epoch: 179. T, V acc: 99.8%, 40.3%. Took: 11.27s\n",
      "Epoch: 180. T, V acc: 99.7%, 40.6%. Took: 11.30s\n",
      "Epoch: 181. T, V acc: 99.8%, 41.0%. Took: 11.30s\n",
      "Epoch: 182. T, V acc: 99.7%, 40.3%. Took: 11.30s\n",
      "Epoch: 183. T, V acc: 99.8%, 40.7%. Took: 11.31s\n",
      "Epoch: 184. T, V acc: 99.8%, 41.1%. Took: 11.33s\n",
      "Epoch: 185. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 186. T, V acc: 99.8%, 41.1%. Took: 11.32s\n",
      "Epoch: 187. T, V acc: 99.8%, 40.7%. Took: 11.30s\n",
      "Epoch: 188. T, V acc: 99.8%, 40.9%. Took: 11.30s\n",
      "Epoch: 189. T, V acc: 99.8%, 40.8%. Took: 11.29s\n",
      "Epoch: 190. T, V acc: 99.8%, 40.9%. Took: 11.24s\n",
      "Epoch: 191. T, V acc: 99.8%, 40.9%. Took: 11.25s\n",
      "Epoch: 192. T, V acc: 99.7%, 40.0%. Took: 11.30s\n",
      "Epoch: 193. T, V acc: 99.5%, 40.5%. Took: 11.23s\n",
      "Epoch: 194. T, V acc: 99.3%, 39.9%. Took: 11.28s\n",
      "Epoch: 195. T, V acc: 99.5%, 40.1%. Took: 11.33s\n",
      "Epoch: 196. T, V acc: 99.4%, 39.9%. Took: 11.27s\n",
      "Epoch: 197. T, V acc: 99.6%, 40.5%. Took: 11.28s\n",
      "Epoch: 198. T, V acc: 99.6%, 40.7%. Took: 11.29s\n",
      "Epoch: 199. T, V acc: 99.7%, 40.2%. Took: 11.24s\n",
      "Epoch: 200. T, V acc: 99.7%, 40.4%. Took: 11.29s\n",
      "Epoch: 201. T, V acc: 99.7%, 40.7%. Took: 11.31s\n",
      "Epoch: 202. T, V acc: 99.8%, 41.0%. Took: 11.32s\n",
      "Epoch: 203. T, V acc: 99.8%, 40.1%. Took: 11.24s\n",
      "Epoch: 204. T, V acc: 99.8%, 41.0%. Took: 11.29s\n",
      "Epoch: 205. T, V acc: 99.7%, 40.6%. Took: 11.25s\n",
      "Epoch: 206. T, V acc: 99.8%, 41.2%. Took: 11.27s\n",
      "Epoch: 207. T, V acc: 99.8%, 41.0%. Took: 11.30s\n",
      "Epoch: 208. T, V acc: 99.8%, 41.0%. Took: 11.23s\n",
      "Epoch: 209. T, V acc: 99.8%, 40.7%. Took: 11.20s\n",
      "Epoch: 210. T, V acc: 99.8%, 41.1%. Took: 11.30s\n",
      "Epoch: 211. T, V acc: 99.8%, 41.0%. Took: 11.21s\n",
      "Epoch: 212. T, V acc: 99.7%, 40.5%. Took: 11.28s\n",
      "Epoch: 213. T, V acc: 99.8%, 41.0%. Took: 11.27s\n",
      "Epoch: 214. T, V acc: 99.8%, 41.3%. Took: 11.29s\n",
      "Epoch: 215. T, V acc: 99.7%, 41.1%. Took: 11.24s\n",
      "Epoch: 216. T, V acc: 99.7%, 40.7%. Took: 11.31s\n",
      "Epoch: 217. T, V acc: 99.7%, 40.3%. Took: 11.32s\n",
      "Epoch: 218. T, V acc: 99.7%, 40.2%. Took: 11.25s\n",
      "Epoch: 219. T, V acc: 99.7%, 41.2%. Took: 11.22s\n",
      "Epoch: 220. T, V acc: 99.8%, 41.1%. Took: 11.22s\n",
      "Epoch: 221. T, V acc: 99.7%, 40.3%. Took: 11.24s\n",
      "Epoch: 222. T, V acc: 99.6%, 40.1%. Took: 11.28s\n",
      "Epoch: 223. T, V acc: 99.4%, 39.9%. Took: 11.27s\n",
      "Epoch: 224. T, V acc: 99.5%, 39.3%. Took: 11.27s\n",
      "Epoch: 225. T, V acc: 99.4%, 39.8%. Took: 11.30s\n",
      "Epoch: 226. T, V acc: 99.6%, 40.7%. Took: 11.26s\n",
      "Epoch: 227. T, V acc: 99.7%, 40.1%. Took: 11.24s\n",
      "Epoch: 228. T, V acc: 99.7%, 40.9%. Took: 11.24s\n",
      "Epoch: 229. T, V acc: 99.7%, 41.0%. Took: 11.25s\n",
      "Epoch: 230. T, V acc: 99.7%, 40.9%. Took: 11.25s\n",
      "Epoch: 231. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 232. T, V acc: 99.8%, 41.1%. Took: 11.25s\n",
      "Epoch: 233. T, V acc: 99.7%, 41.1%. Took: 11.28s\n",
      "Epoch: 234. T, V acc: 99.8%, 41.0%. Took: 11.26s\n",
      "Epoch: 235. T, V acc: 99.8%, 41.2%. Took: 11.25s\n",
      "Epoch: 236. T, V acc: 99.8%, 41.4%. Took: 11.27s\n",
      "Epoch: 237. T, V acc: 99.8%, 41.4%. Took: 11.31s\n",
      "Epoch: 238. T, V acc: 99.8%, 41.5%. Took: 11.30s\n",
      "Epoch: 239. T, V acc: 99.7%, 41.1%. Took: 11.28s\n",
      "Epoch: 240. T, V acc: 99.7%, 41.0%. Took: 11.30s\n",
      "Epoch: 241. T, V acc: 99.7%, 40.7%. Took: 11.26s\n",
      "Epoch: 242. T, V acc: 99.7%, 41.1%. Took: 11.25s\n",
      "Epoch: 243. T, V acc: 99.7%, 40.5%. Took: 11.29s\n",
      "Epoch: 244. T, V acc: 99.7%, 41.2%. Took: 11.28s\n",
      "Epoch: 245. T, V acc: 99.8%, 41.5%. Took: 11.27s\n",
      "Epoch: 246. T, V acc: 99.8%, 41.4%. Took: 11.33s\n",
      "Epoch: 247. T, V acc: 99.7%, 41.5%. Took: 11.29s\n",
      "Epoch: 248. T, V acc: 99.7%, 40.7%. Took: 11.29s\n",
      "Epoch: 249. T, V acc: 99.7%, 40.9%. Took: 11.30s\n",
      "Epoch: 250. T, V acc: 99.7%, 40.9%. Took: 11.33s\n",
      "Epoch: 251. T, V acc: 99.7%, 40.0%. Took: 11.22s\n",
      "Epoch: 252. T, V acc: 99.7%, 40.9%. Took: 11.24s\n",
      "Epoch: 253. T, V acc: 99.7%, 40.3%. Took: 11.23s\n",
      "Epoch: 254. T, V acc: 99.7%, 40.8%. Took: 11.22s\n",
      "Epoch: 255. T, V acc: 99.7%, 40.4%. Took: 11.22s\n",
      "Epoch: 256. T, V acc: 99.7%, 39.7%. Took: 11.24s\n",
      "Epoch: 257. T, V acc: 99.7%, 40.3%. Took: 11.22s\n",
      "Epoch: 258. T, V acc: 99.8%, 40.4%. Took: 11.20s\n",
      "Epoch: 259. T, V acc: 99.8%, 40.5%. Took: 11.23s\n",
      "Epoch: 260. T, V acc: 99.8%, 40.1%. Took: 11.26s\n",
      "Epoch: 261. T, V acc: 99.8%, 40.4%. Took: 11.25s\n",
      "Epoch: 262. T, V acc: 99.8%, 40.5%. Took: 11.29s\n",
      "Epoch: 263. T, V acc: 99.8%, 40.4%. Took: 11.27s\n",
      "Epoch: 264. T, V acc: 99.6%, 40.3%. Took: 11.35s\n",
      "Epoch: 265. T, V acc: 99.7%, 40.8%. Took: 11.33s\n",
      "Epoch: 266. T, V acc: 99.7%, 40.9%. Took: 11.25s\n",
      "Epoch: 267. T, V acc: 99.7%, 40.0%. Took: 11.27s\n",
      "Epoch: 268. T, V acc: 99.7%, 40.5%. Took: 11.29s\n",
      "Epoch: 269. T, V acc: 99.8%, 40.4%. Took: 11.26s\n",
      "Epoch: 270. T, V acc: 99.8%, 40.7%. Took: 11.21s\n",
      "Epoch: 271. T, V acc: 99.8%, 40.4%. Took: 11.25s\n",
      "Epoch: 272. T, V acc: 99.8%, 41.0%. Took: 11.27s\n",
      "Epoch: 273. T, V acc: 99.8%, 41.3%. Took: 11.21s\n",
      "Epoch: 274. T, V acc: 99.8%, 41.3%. Took: 11.26s\n",
      "Epoch: 275. T, V acc: 99.8%, 41.4%. Took: 11.21s\n",
      "Epoch: 276. T, V acc: 99.8%, 41.4%. Took: 11.26s\n",
      "Epoch: 277. T, V acc: 99.8%, 41.0%. Took: 11.26s\n",
      "Epoch: 278. T, V acc: 99.8%, 40.8%. Took: 11.24s\n",
      "Epoch: 279. T, V acc: 99.8%, 40.8%. Took: 11.27s\n",
      "Epoch: 280. T, V acc: 99.7%, 40.7%. Took: 11.29s\n",
      "Epoch: 281. T, V acc: 99.7%, 40.0%. Took: 11.22s\n",
      "Epoch: 282. T, V acc: 99.7%, 40.4%. Took: 11.25s\n",
      "Epoch: 283. T, V acc: 99.8%, 40.5%. Took: 11.30s\n",
      "Epoch: 284. T, V acc: 99.8%, 40.8%. Took: 11.27s\n",
      "Epoch: 285. T, V acc: 99.8%, 41.0%. Took: 11.25s\n",
      "Epoch: 286. T, V acc: 99.8%, 40.4%. Took: 11.27s\n",
      "Epoch: 287. T, V acc: 99.8%, 40.8%. Took: 11.22s\n",
      "Epoch: 288. T, V acc: 99.8%, 40.7%. Took: 11.27s\n",
      "Epoch: 289. T, V acc: 99.8%, 40.8%. Took: 11.20s\n",
      "Epoch: 290. T, V acc: 99.8%, 40.2%. Took: 11.26s\n",
      "Epoch: 291. T, V acc: 99.8%, 40.7%. Took: 11.29s\n",
      "Epoch: 292. T, V acc: 99.8%, 40.6%. Took: 11.26s\n",
      "Epoch: 293. T, V acc: 99.8%, 40.5%. Took: 11.25s\n",
      "Epoch: 294. T, V acc: 99.8%, 40.9%. Took: 11.26s\n",
      "Epoch: 295. T, V acc: 99.8%, 40.8%. Took: 11.22s\n",
      "Epoch: 296. T, V acc: 99.8%, 39.7%. Took: 11.21s\n",
      "Epoch: 297. T, V acc: 99.8%, 40.3%. Took: 11.25s\n",
      "Epoch: 298. T, V acc: 99.7%, 40.2%. Took: 11.26s\n",
      "Epoch: 299. T, V acc: 99.7%, 40.1%. Took: 11.22s\n",
      "Epoch: 300. T, V acc: 99.7%, 39.8%. Took: 11.15s\n",
      "Epoch: 301. T, V acc: 99.7%, 40.8%. Took: 11.10s\n",
      "Epoch: 302. T, V acc: 99.7%, 40.6%. Took: 11.09s\n",
      "Epoch: 303. T, V acc: 99.8%, 40.7%. Took: 11.10s\n",
      "Epoch: 304. T, V acc: 99.8%, 40.9%. Took: 11.07s\n",
      "Epoch: 305. T, V acc: 99.8%, 40.6%. Took: 11.06s\n",
      "Epoch: 306. T, V acc: 99.8%, 41.1%. Took: 11.11s\n",
      "Epoch: 307. T, V acc: 99.8%, 41.0%. Took: 11.09s\n",
      "Epoch: 308. T, V acc: 99.8%, 40.8%. Took: 11.09s\n",
      "Epoch: 309. T, V acc: 99.8%, 41.2%. Took: 11.09s\n",
      "Epoch: 310. T, V acc: 99.8%, 41.0%. Took: 11.10s\n",
      "Epoch: 311. T, V acc: 99.8%, 41.1%. Took: 11.10s\n",
      "Epoch: 312. T, V acc: 99.8%, 40.5%. Took: 11.11s\n",
      "Epoch: 313. T, V acc: 99.8%, 40.1%. Took: 11.24s\n",
      "Epoch: 314. T, V acc: 99.8%, 40.0%. Took: 11.24s\n",
      "Epoch: 315. T, V acc: 99.8%, 40.5%. Took: 11.29s\n",
      "Epoch: 316. T, V acc: 99.7%, 40.9%. Took: 11.29s\n",
      "Epoch: 317. T, V acc: 99.5%, 40.3%. Took: 11.28s\n",
      "Epoch: 318. T, V acc: 99.6%, 39.9%. Took: 11.29s\n",
      "Epoch: 319. T, V acc: 99.6%, 40.5%. Took: 11.27s\n",
      "Epoch: 320. T, V acc: 99.7%, 40.1%. Took: 11.26s\n",
      "Epoch: 321. T, V acc: 99.7%, 40.8%. Took: 11.23s\n",
      "Epoch: 322. T, V acc: 99.8%, 41.0%. Took: 11.29s\n",
      "Epoch: 323. T, V acc: 99.8%, 41.2%. Took: 11.24s\n",
      "Epoch: 324. T, V acc: 99.8%, 40.4%. Took: 11.25s\n",
      "Epoch: 325. T, V acc: 99.8%, 40.4%. Took: 11.22s\n",
      "Epoch: 326. T, V acc: 99.8%, 40.8%. Took: 11.24s\n",
      "Epoch: 327. T, V acc: 99.8%, 41.0%. Took: 11.24s\n",
      "Epoch: 328. T, V acc: 99.8%, 41.1%. Took: 11.25s\n",
      "Epoch: 329. T, V acc: 99.8%, 41.2%. Took: 11.25s\n",
      "Epoch: 330. T, V acc: 99.8%, 42.0%. Took: 11.25s\n",
      "Epoch: 331. T, V acc: 99.8%, 41.5%. Took: 11.23s\n",
      "Epoch: 332. T, V acc: 99.8%, 41.2%. Took: 11.24s\n",
      "Epoch: 333. T, V acc: 99.8%, 40.7%. Took: 11.24s\n",
      "Epoch: 334. T, V acc: 99.8%, 40.9%. Took: 11.30s\n",
      "Epoch: 335. T, V acc: 99.8%, 41.0%. Took: 11.33s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 336. T, V acc: 99.8%, 41.1%. Took: 11.30s\n",
      "Epoch: 337. T, V acc: 99.8%, 41.1%. Took: 11.27s\n",
      "Epoch: 338. T, V acc: 99.8%, 41.2%. Took: 11.23s\n",
      "Epoch: 339. T, V acc: 99.8%, 41.2%. Took: 11.24s\n",
      "Epoch: 340. T, V acc: 99.8%, 41.2%. Took: 11.26s\n",
      "Epoch: 341. T, V acc: 99.8%, 41.4%. Took: 11.24s\n",
      "Epoch: 342. T, V acc: 99.8%, 41.5%. Took: 11.25s\n",
      "Epoch: 343. T, V acc: 99.8%, 41.6%. Took: 11.28s\n",
      "Epoch: 344. T, V acc: 99.8%, 40.8%. Took: 11.26s\n",
      "Epoch: 345. T, V acc: 99.8%, 40.6%. Took: 11.27s\n",
      "Epoch: 346. T, V acc: 99.8%, 40.9%. Took: 11.29s\n",
      "Epoch: 347. T, V acc: 99.8%, 40.9%. Took: 11.28s\n",
      "Epoch: 348. T, V acc: 99.6%, 40.6%. Took: 11.30s\n",
      "Epoch: 349. T, V acc: 99.6%, 40.4%. Took: 11.36s\n",
      "Epoch: 350. T, V acc: 99.7%, 40.6%. Took: 11.31s\n",
      "Epoch: 351. T, V acc: 99.8%, 41.1%. Took: 11.31s\n",
      "Epoch: 352. T, V acc: 99.8%, 40.9%. Took: 11.24s\n",
      "Epoch: 353. T, V acc: 99.8%, 41.3%. Took: 11.27s\n",
      "Epoch: 354. T, V acc: 99.8%, 41.1%. Took: 11.27s\n",
      "Epoch: 355. T, V acc: 99.8%, 41.4%. Took: 11.27s\n",
      "Epoch: 356. T, V acc: 99.8%, 41.1%. Took: 11.32s\n",
      "Epoch: 357. T, V acc: 99.8%, 41.1%. Took: 11.28s\n",
      "Epoch: 358. T, V acc: 99.8%, 41.2%. Took: 11.27s\n",
      "Epoch: 359. T, V acc: 99.8%, 41.2%. Took: 11.32s\n",
      "Epoch: 360. T, V acc: 99.8%, 41.1%. Took: 11.32s\n",
      "Epoch: 361. T, V acc: 99.8%, 41.2%. Took: 11.33s\n",
      "Epoch: 362. T, V acc: 99.8%, 41.0%. Took: 11.31s\n",
      "Epoch: 363. T, V acc: 99.8%, 41.1%. Took: 11.31s\n",
      "Epoch: 364. T, V acc: 99.8%, 41.2%. Took: 11.32s\n",
      "Epoch: 365. T, V acc: 99.8%, 41.3%. Took: 11.31s\n",
      "Epoch: 366. T, V acc: 99.8%, 41.3%. Took: 11.32s\n",
      "Epoch: 367. T, V acc: 99.8%, 41.2%. Took: 11.31s\n",
      "Epoch: 368. T, V acc: 99.8%, 41.0%. Took: 11.23s\n",
      "Epoch: 369. T, V acc: 99.8%, 41.3%. Took: 11.26s\n",
      "Epoch: 370. T, V acc: 99.8%, 40.5%. Took: 11.11s\n",
      "Epoch: 371. T, V acc: 99.8%, 40.6%. Took: 11.11s\n",
      "Epoch: 372. T, V acc: 99.7%, 40.2%. Took: 11.13s\n",
      "Epoch: 373. T, V acc: 99.7%, 40.2%. Took: 11.33s\n",
      "Epoch: 374. T, V acc: 99.7%, 39.7%. Took: 11.35s\n",
      "Epoch: 375. T, V acc: 99.7%, 41.0%. Took: 11.32s\n",
      "Epoch: 376. T, V acc: 99.8%, 40.9%. Took: 11.36s\n",
      "Epoch: 377. T, V acc: 99.8%, 40.5%. Took: 11.31s\n",
      "Epoch: 378. T, V acc: 99.8%, 40.6%. Took: 11.29s\n",
      "Epoch: 379. T, V acc: 99.8%, 40.4%. Took: 11.27s\n",
      "Epoch: 380. T, V acc: 99.8%, 40.9%. Took: 11.30s\n",
      "Epoch: 381. T, V acc: 99.8%, 41.0%. Took: 11.31s\n",
      "Epoch: 382. T, V acc: 99.8%, 40.7%. Took: 11.32s\n",
      "Epoch: 383. T, V acc: 99.8%, 40.6%. Took: 11.34s\n",
      "Epoch: 384. T, V acc: 99.8%, 40.9%. Took: 11.31s\n",
      "Epoch: 385. T, V acc: 99.8%, 40.6%. Took: 11.31s\n",
      "Epoch: 386. T, V acc: 99.8%, 41.0%. Took: 11.29s\n",
      "Epoch: 387. T, V acc: 99.8%, 41.3%. Took: 11.31s\n",
      "Epoch: 388. T, V acc: 99.8%, 41.1%. Took: 11.35s\n",
      "Epoch: 389. T, V acc: 99.8%, 40.7%. Took: 11.32s\n",
      "Epoch: 390. T, V acc: 99.8%, 41.2%. Took: 11.36s\n",
      "Epoch: 391. T, V acc: 99.8%, 40.9%. Took: 11.28s\n",
      "Epoch: 392. T, V acc: 99.8%, 40.4%. Took: 11.29s\n",
      "Epoch: 393. T, V acc: 99.8%, 40.6%. Took: 11.32s\n",
      "Epoch: 394. T, V acc: 99.8%, 41.4%. Took: 11.32s\n",
      "Epoch: 395. T, V acc: 99.8%, 41.0%. Took: 11.32s\n",
      "Epoch: 396. T, V acc: 99.8%, 40.6%. Took: 11.28s\n",
      "Epoch: 397. T, V acc: 99.8%, 40.7%. Took: 11.27s\n",
      "Epoch: 398. T, V acc: 99.8%, 41.2%. Took: 11.25s\n",
      "Epoch: 399. T, V acc: 99.8%, 41.0%. Took: 11.38s\n",
      "Epoch: 400. T, V acc: 99.8%, 40.7%. Took: 11.32s\n",
      "Epoch: 401. T, V acc: 99.8%, 40.7%. Took: 11.33s\n",
      "Epoch: 402. T, V acc: 99.8%, 40.6%. Took: 11.26s\n",
      "Epoch: 403. T, V acc: 99.8%, 40.5%. Took: 11.31s\n",
      "Epoch: 404. T, V acc: 99.8%, 41.0%. Took: 11.30s\n",
      "Epoch: 405. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 406. T, V acc: 99.8%, 40.4%. Took: 11.32s\n",
      "Epoch: 407. T, V acc: 99.8%, 40.8%. Took: 11.30s\n",
      "Epoch: 408. T, V acc: 99.7%, 40.3%. Took: 11.31s\n",
      "Epoch: 409. T, V acc: 99.7%, 40.0%. Took: 11.25s\n",
      "Epoch: 410. T, V acc: 99.8%, 40.2%. Took: 11.21s\n",
      "Epoch: 411. T, V acc: 99.8%, 40.4%. Took: 11.28s\n",
      "Epoch: 412. T, V acc: 99.8%, 41.0%. Took: 11.27s\n",
      "Epoch: 413. T, V acc: 99.8%, 40.8%. Took: 11.29s\n",
      "Epoch: 414. T, V acc: 99.8%, 40.6%. Took: 11.31s\n",
      "Epoch: 415. T, V acc: 99.8%, 40.7%. Took: 11.23s\n",
      "Epoch: 416. T, V acc: 99.8%, 40.4%. Took: 11.23s\n",
      "Epoch: 417. T, V acc: 99.8%, 40.0%. Took: 11.21s\n",
      "Epoch: 418. T, V acc: 99.8%, 40.5%. Took: 11.23s\n",
      "Epoch: 419. T, V acc: 99.8%, 41.0%. Took: 11.24s\n",
      "Epoch: 420. T, V acc: 99.8%, 40.6%. Took: 11.21s\n",
      "Epoch: 421. T, V acc: 99.8%, 40.9%. Took: 11.22s\n",
      "Epoch: 422. T, V acc: 99.8%, 41.1%. Took: 11.21s\n",
      "Epoch: 423. T, V acc: 99.8%, 40.7%. Took: 11.24s\n",
      "Epoch: 424. T, V acc: 99.8%, 40.3%. Took: 11.20s\n",
      "Epoch: 425. T, V acc: 99.8%, 40.6%. Took: 11.32s\n",
      "Epoch: 426. T, V acc: 99.8%, 41.0%. Took: 11.33s\n",
      "Epoch: 427. T, V acc: 99.8%, 41.5%. Took: 11.32s\n",
      "Epoch: 428. T, V acc: 99.8%, 41.5%. Took: 11.30s\n",
      "Epoch: 429. T, V acc: 99.8%, 41.3%. Took: 11.30s\n",
      "Epoch: 430. T, V acc: 99.8%, 41.2%. Took: 11.30s\n",
      "Epoch: 431. T, V acc: 99.8%, 41.4%. Took: 11.28s\n",
      "Epoch: 432. T, V acc: 99.9%, 41.5%. Took: 11.31s\n",
      "Epoch: 433. T, V acc: 99.8%, 41.6%. Took: 11.28s\n",
      "Epoch: 434. T, V acc: 99.8%, 41.4%. Took: 11.28s\n",
      "Epoch: 435. T, V acc: 99.8%, 41.3%. Took: 11.22s\n",
      "Epoch: 436. T, V acc: 99.8%, 41.4%. Took: 11.26s\n",
      "Epoch: 437. T, V acc: 99.9%, 41.7%. Took: 11.29s\n",
      "Epoch: 438. T, V acc: 99.8%, 41.9%. Took: 11.32s\n",
      "Epoch: 439. T, V acc: 99.9%, 41.7%. Took: 11.32s\n",
      "Epoch: 440. T, V acc: 99.8%, 41.8%. Took: 11.33s\n",
      "Epoch: 441. T, V acc: 99.9%, 41.9%. Took: 11.32s\n",
      "Epoch: 442. T, V acc: 99.8%, 41.8%. Took: 11.27s\n",
      "Epoch: 443. T, V acc: 99.8%, 41.7%. Took: 11.18s\n",
      "Epoch: 444. T, V acc: 99.9%, 41.7%. Took: 11.11s\n",
      "Epoch: 445. T, V acc: 99.8%, 41.8%. Took: 11.10s\n",
      "Epoch: 446. T, V acc: 99.8%, 41.7%. Took: 11.11s\n",
      "Epoch: 447. T, V acc: 99.8%, 41.4%. Took: 11.10s\n",
      "Epoch: 448. T, V acc: 99.8%, 40.8%. Took: 11.12s\n",
      "Epoch: 449. T, V acc: 99.8%, 40.7%. Took: 11.06s\n",
      "Epoch: 450. T, V acc: 99.6%, 40.8%. Took: 11.11s\n",
      "Epoch: 451. T, V acc: 99.7%, 40.9%. Took: 11.25s\n",
      "Epoch: 452. T, V acc: 99.6%, 40.0%. Took: 11.24s\n",
      "Epoch: 453. T, V acc: 99.8%, 40.6%. Took: 11.26s\n",
      "Epoch: 454. T, V acc: 99.7%, 40.9%. Took: 11.21s\n",
      "Epoch: 455. T, V acc: 99.8%, 40.2%. Took: 11.22s\n",
      "Epoch: 456. T, V acc: 99.8%, 40.5%. Took: 11.24s\n",
      "Epoch: 457. T, V acc: 99.8%, 40.5%. Took: 11.20s\n",
      "Epoch: 458. T, V acc: 99.8%, 40.8%. Took: 11.28s\n",
      "Epoch: 459. T, V acc: 99.8%, 41.2%. Took: 11.36s\n",
      "Epoch: 460. T, V acc: 99.8%, 40.6%. Took: 11.34s\n",
      "Epoch: 461. T, V acc: 99.8%, 40.8%. Took: 11.30s\n",
      "Epoch: 462. T, V acc: 99.8%, 40.6%. Took: 11.29s\n",
      "Epoch: 463. T, V acc: 99.8%, 40.6%. Took: 11.17s\n",
      "Epoch: 464. T, V acc: 99.8%, 40.6%. Took: 11.09s\n",
      "Epoch: 465. T, V acc: 99.8%, 40.6%. Took: 11.07s\n",
      "Epoch: 466. T, V acc: 99.8%, 40.7%. Took: 11.08s\n",
      "Epoch: 467. T, V acc: 99.8%, 40.5%. Took: 11.24s\n",
      "Epoch: 468. T, V acc: 99.8%, 40.9%. Took: 11.26s\n",
      "Epoch: 469. T, V acc: 99.8%, 40.7%. Took: 11.15s\n",
      "Epoch: 470. T, V acc: 99.8%, 40.9%. Took: 11.11s\n",
      "Epoch: 471. T, V acc: 99.8%, 41.0%. Took: 11.20s\n",
      "Epoch: 472. T, V acc: 99.8%, 40.9%. Took: 11.29s\n",
      "Epoch: 473. T, V acc: 99.8%, 41.0%. Took: 11.27s\n",
      "Epoch: 474. T, V acc: 99.8%, 40.8%. Took: 11.17s\n",
      "Epoch: 475. T, V acc: 99.8%, 40.4%. Took: 11.27s\n",
      "Epoch: 476. T, V acc: 99.8%, 40.6%. Took: 11.33s\n",
      "Epoch: 477. T, V acc: 99.8%, 40.4%. Took: 11.25s\n",
      "Epoch: 478. T, V acc: 99.8%, 40.5%. Took: 11.30s\n",
      "Epoch: 479. T, V acc: 99.8%, 40.5%. Took: 11.30s\n",
      "Epoch: 480. T, V acc: 99.8%, 40.6%. Took: 11.33s\n",
      "Epoch: 481. T, V acc: 99.8%, 40.6%. Took: 11.31s\n",
      "Epoch: 482. T, V acc: 99.8%, 40.3%. Took: 11.28s\n",
      "Epoch: 483. T, V acc: 99.8%, 41.0%. Took: 11.30s\n",
      "Epoch: 484. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 485. T, V acc: 99.8%, 40.0%. Took: 11.30s\n",
      "Epoch: 486. T, V acc: 99.8%, 40.9%. Took: 11.35s\n",
      "Epoch: 487. T, V acc: 99.8%, 40.6%. Took: 11.35s\n",
      "Epoch: 488. T, V acc: 99.8%, 40.5%. Took: 11.34s\n",
      "Epoch: 489. T, V acc: 99.8%, 40.8%. Took: 11.31s\n",
      "Epoch: 490. T, V acc: 99.8%, 41.2%. Took: 11.25s\n",
      "Epoch: 491. T, V acc: 99.8%, 41.1%. Took: 11.23s\n",
      "Epoch: 492. T, V acc: 99.8%, 40.8%. Took: 11.21s\n",
      "Epoch: 493. T, V acc: 99.8%, 40.7%. Took: 11.17s\n",
      "Epoch: 494. T, V acc: 99.8%, 41.2%. Took: 11.23s\n",
      "Epoch: 495. T, V acc: 99.8%, 40.9%. Took: 11.25s\n",
      "Epoch: 496. T, V acc: 99.8%, 40.4%. Took: 11.25s\n",
      "Epoch: 497. T, V acc: 99.8%, 40.9%. Took: 11.26s\n",
      "Epoch: 498. T, V acc: 99.8%, 40.9%. Took: 11.23s\n",
      "Epoch: 499. T, V acc: 99.8%, 41.2%. Took: 11.22s\n",
      "Epoch: 500. T, V acc: 99.8%, 41.1%. Took: 11.21s\n",
      "Epoch: 501. T, V acc: 99.8%, 41.4%. Took: 11.24s\n",
      "Epoch: 502. T, V acc: 99.8%, 41.9%. Took: 11.23s\n",
      "Epoch: 503. T, V acc: 99.8%, 41.7%. Took: 11.22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 504. T, V acc: 99.8%, 41.0%. Took: 11.26s\n",
      "Epoch: 505. T, V acc: 99.8%, 41.8%. Took: 11.23s\n",
      "Epoch: 506. T, V acc: 99.8%, 41.6%. Took: 11.25s\n",
      "Epoch: 507. T, V acc: 99.8%, 40.7%. Took: 11.25s\n",
      "Epoch: 508. T, V acc: 99.8%, 41.0%. Took: 11.27s\n",
      "Epoch: 509. T, V acc: 99.8%, 41.5%. Took: 11.25s\n",
      "Epoch: 510. T, V acc: 99.8%, 40.9%. Took: 11.23s\n",
      "Epoch: 511. T, V acc: 99.8%, 41.0%. Took: 11.25s\n",
      "Epoch: 512. T, V acc: 99.8%, 41.3%. Took: 11.20s\n",
      "Epoch: 513. T, V acc: 99.8%, 41.3%. Took: 11.11s\n",
      "Epoch: 514. T, V acc: 99.8%, 41.0%. Took: 11.07s\n",
      "Epoch: 515. T, V acc: 99.8%, 41.2%. Took: 11.10s\n",
      "Epoch: 516. T, V acc: 99.8%, 41.3%. Took: 11.08s\n",
      "Epoch: 517. T, V acc: 99.8%, 40.5%. Took: 11.10s\n",
      "Epoch: 518. T, V acc: 99.8%, 41.5%. Took: 11.08s\n",
      "Epoch: 519. T, V acc: 99.8%, 41.5%. Took: 11.09s\n",
      "Epoch: 520. T, V acc: 99.8%, 41.3%. Took: 11.10s\n",
      "Epoch: 521. T, V acc: 99.8%, 41.5%. Took: 11.11s\n",
      "Epoch: 522. T, V acc: 99.8%, 40.9%. Took: 11.17s\n",
      "Epoch: 523. T, V acc: 99.8%, 41.4%. Took: 11.24s\n",
      "Epoch: 524. T, V acc: 99.8%, 41.5%. Took: 11.27s\n",
      "Epoch: 525. T, V acc: 99.8%, 41.6%. Took: 11.27s\n",
      "Epoch: 526. T, V acc: 99.8%, 41.6%. Took: 11.25s\n",
      "Epoch: 527. T, V acc: 99.8%, 41.5%. Took: 11.22s\n",
      "Epoch: 528. T, V acc: 99.8%, 41.3%. Took: 11.27s\n",
      "Epoch: 529. T, V acc: 99.8%, 41.5%. Took: 11.26s\n",
      "Epoch: 530. T, V acc: 99.8%, 41.6%. Took: 11.27s\n",
      "Epoch: 531. T, V acc: 99.8%, 41.4%. Took: 11.27s\n",
      "Epoch: 532. T, V acc: 99.9%, 41.5%. Took: 11.25s\n",
      "Epoch: 533. T, V acc: 99.8%, 41.5%. Took: 11.28s\n",
      "Epoch: 534. T, V acc: 99.8%, 41.4%. Took: 11.30s\n",
      "Epoch: 535. T, V acc: 99.8%, 41.1%. Took: 11.31s\n",
      "Epoch: 536. T, V acc: 99.8%, 39.7%. Took: 11.31s\n",
      "Epoch: 537. T, V acc: 99.8%, 40.7%. Took: 11.31s\n",
      "Epoch: 538. T, V acc: 99.8%, 40.8%. Took: 11.29s\n",
      "Epoch: 539. T, V acc: 99.8%, 40.4%. Took: 11.31s\n",
      "Epoch: 540. T, V acc: 99.8%, 41.2%. Took: 11.31s\n",
      "Epoch: 541. T, V acc: 99.8%, 40.9%. Took: 11.29s\n",
      "Epoch: 542. T, V acc: 99.8%, 40.9%. Took: 11.25s\n",
      "Epoch: 543. T, V acc: 99.8%, 40.9%. Took: 11.32s\n",
      "Epoch: 544. T, V acc: 99.8%, 40.8%. Took: 11.31s\n",
      "Epoch: 545. T, V acc: 99.8%, 40.8%. Took: 11.27s\n",
      "Epoch: 546. T, V acc: 99.8%, 40.8%. Took: 11.29s\n",
      "Epoch: 547. T, V acc: 99.8%, 40.7%. Took: 11.32s\n",
      "Epoch: 548. T, V acc: 99.8%, 41.0%. Took: 11.27s\n",
      "Epoch: 549. T, V acc: 99.8%, 40.4%. Took: 11.25s\n",
      "Epoch: 550. T, V acc: 99.8%, 40.9%. Took: 11.24s\n",
      "Epoch: 551. T, V acc: 99.8%, 40.3%. Took: 11.23s\n",
      "Epoch: 552. T, V acc: 99.8%, 40.0%. Took: 11.24s\n",
      "Epoch: 553. T, V acc: 99.8%, 40.5%. Took: 11.27s\n",
      "Epoch: 554. T, V acc: 99.8%, 40.3%. Took: 11.23s\n",
      "Epoch: 555. T, V acc: 99.8%, 40.9%. Took: 11.27s\n",
      "Epoch: 556. T, V acc: 99.8%, 40.5%. Took: 11.26s\n",
      "Epoch: 557. T, V acc: 99.8%, 40.6%. Took: 11.23s\n",
      "Epoch: 558. T, V acc: 99.8%, 40.5%. Took: 11.29s\n",
      "Epoch: 559. T, V acc: 99.8%, 40.8%. Took: 11.24s\n",
      "Epoch: 560. T, V acc: 99.8%, 39.8%. Took: 11.29s\n",
      "Epoch: 561. T, V acc: 99.8%, 41.0%. Took: 11.28s\n",
      "Epoch: 562. T, V acc: 99.8%, 41.1%. Took: 11.26s\n",
      "Epoch: 563. T, V acc: 99.8%, 40.6%. Took: 11.26s\n",
      "Epoch: 564. T, V acc: 99.8%, 40.5%. Took: 11.30s\n",
      "Epoch: 565. T, V acc: 99.8%, 40.7%. Took: 11.29s\n",
      "Epoch: 566. T, V acc: 99.8%, 40.3%. Took: 11.25s\n",
      "Epoch: 567. T, V acc: 99.8%, 40.5%. Took: 11.22s\n",
      "Epoch: 568. T, V acc: 99.8%, 40.4%. Took: 11.23s\n",
      "Epoch: 569. T, V acc: 99.8%, 41.0%. Took: 11.31s\n",
      "Epoch: 570. T, V acc: 99.8%, 40.8%. Took: 11.33s\n",
      "Epoch: 571. T, V acc: 99.8%, 40.7%. Took: 11.32s\n",
      "Epoch: 572. T, V acc: 99.8%, 41.0%. Took: 11.32s\n",
      "Epoch: 573. T, V acc: 99.8%, 40.7%. Took: 11.29s\n",
      "Epoch: 574. T, V acc: 99.8%, 40.7%. Took: 11.29s\n",
      "Epoch: 575. T, V acc: 99.8%, 41.1%. Took: 11.30s\n",
      "Epoch: 576. T, V acc: 99.8%, 41.3%. Took: 11.26s\n",
      "Epoch: 577. T, V acc: 99.8%, 41.1%. Took: 11.29s\n",
      "Epoch: 578. T, V acc: 99.8%, 40.9%. Took: 11.30s\n",
      "Epoch: 579. T, V acc: 99.8%, 41.0%. Took: 11.25s\n",
      "Epoch: 580. T, V acc: 99.8%, 40.9%. Took: 11.25s\n",
      "Epoch: 581. T, V acc: 99.8%, 41.0%. Took: 11.23s\n",
      "Epoch: 582. T, V acc: 99.8%, 41.2%. Took: 11.23s\n",
      "Epoch: 583. T, V acc: 99.8%, 41.1%. Took: 11.24s\n",
      "Epoch: 584. T, V acc: 99.8%, 41.2%. Took: 11.27s\n",
      "Epoch: 585. T, V acc: 99.8%, 41.2%. Took: 11.32s\n",
      "Epoch: 586. T, V acc: 99.8%, 41.6%. Took: 11.29s\n",
      "Epoch: 587. T, V acc: 99.8%, 41.3%. Took: 11.29s\n",
      "Epoch: 588. T, V acc: 99.9%, 41.1%. Took: 11.29s\n",
      "Epoch: 589. T, V acc: 99.8%, 41.0%. Took: 11.22s\n",
      "Epoch: 590. T, V acc: 99.9%, 41.6%. Took: 11.25s\n",
      "Epoch: 591. T, V acc: 99.8%, 40.6%. Took: 11.24s\n",
      "Epoch: 592. T, V acc: 99.8%, 41.4%. Took: 11.27s\n",
      "Epoch: 593. T, V acc: 99.8%, 41.0%. Took: 11.27s\n",
      "Epoch: 594. T, V acc: 99.8%, 41.3%. Took: 11.32s\n",
      "Epoch: 595. T, V acc: 99.8%, 41.2%. Took: 11.29s\n",
      "Epoch: 596. T, V acc: 99.8%, 41.5%. Took: 11.25s\n",
      "Epoch: 597. T, V acc: 99.8%, 41.6%. Took: 11.25s\n",
      "Epoch: 598. T, V acc: 99.8%, 41.5%. Took: 11.24s\n",
      "Epoch: 599. T, V acc: 99.8%, 41.0%. Took: 11.27s\n",
      "Epoch: 600. T, V acc: 99.8%, 41.2%. Took: 11.27s\n",
      "Epoch: 601. T, V acc: 99.8%, 41.4%. Took: 11.32s\n",
      "Epoch: 602. T, V acc: 99.8%, 41.8%. Took: 11.29s\n",
      "Epoch: 603. T, V acc: 99.8%, 41.1%. Took: 11.21s\n",
      "Epoch: 604. T, V acc: 99.8%, 41.6%. Took: 11.33s\n",
      "Epoch: 605. T, V acc: 99.8%, 41.1%. Took: 11.30s\n",
      "Epoch: 606. T, V acc: 99.8%, 41.2%. Took: 11.29s\n",
      "Epoch: 607. T, V acc: 99.8%, 40.3%. Took: 11.31s\n",
      "Epoch: 608. T, V acc: 99.8%, 41.5%. Took: 11.27s\n",
      "Epoch: 609. T, V acc: 99.8%, 41.3%. Took: 11.35s\n",
      "Epoch: 610. T, V acc: 99.8%, 41.5%. Took: 11.31s\n",
      "Epoch: 611. T, V acc: 99.8%, 40.5%. Took: 11.32s\n",
      "Epoch: 612. T, V acc: 99.8%, 40.1%. Took: 11.33s\n",
      "Epoch: 613. T, V acc: 99.8%, 40.9%. Took: 11.30s\n",
      "Epoch: 614. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 615. T, V acc: 99.8%, 41.3%. Took: 11.31s\n",
      "Epoch: 616. T, V acc: 99.8%, 41.1%. Took: 11.28s\n",
      "Epoch: 617. T, V acc: 99.8%, 41.1%. Took: 11.27s\n",
      "Epoch: 618. T, V acc: 99.8%, 40.9%. Took: 11.30s\n",
      "Epoch: 619. T, V acc: 99.8%, 40.8%. Took: 11.31s\n",
      "Epoch: 620. T, V acc: 99.8%, 40.3%. Took: 11.30s\n",
      "Epoch: 621. T, V acc: 99.8%, 40.4%. Took: 11.29s\n",
      "Epoch: 622. T, V acc: 99.8%, 40.7%. Took: 11.31s\n",
      "Epoch: 623. T, V acc: 99.8%, 40.9%. Took: 11.27s\n",
      "Epoch: 624. T, V acc: 99.8%, 40.9%. Took: 11.32s\n",
      "Epoch: 625. T, V acc: 99.8%, 41.0%. Took: 11.26s\n",
      "Epoch: 626. T, V acc: 99.8%, 41.1%. Took: 11.25s\n",
      "Epoch: 627. T, V acc: 99.8%, 41.3%. Took: 11.28s\n",
      "Epoch: 628. T, V acc: 99.8%, 41.3%. Took: 11.27s\n",
      "Epoch: 629. T, V acc: 99.8%, 41.6%. Took: 11.29s\n",
      "Epoch: 630. T, V acc: 99.8%, 41.3%. Took: 11.28s\n",
      "Epoch: 631. T, V acc: 99.8%, 41.4%. Took: 11.26s\n",
      "Epoch: 632. T, V acc: 99.8%, 41.3%. Took: 11.27s\n",
      "Epoch: 633. T, V acc: 99.8%, 41.5%. Took: 11.31s\n",
      "Epoch: 634. T, V acc: 99.8%, 41.3%. Took: 11.30s\n",
      "Epoch: 635. T, V acc: 99.8%, 41.5%. Took: 11.23s\n",
      "Epoch: 636. T, V acc: 99.8%, 40.7%. Took: 11.29s\n",
      "Epoch: 637. T, V acc: 99.8%, 40.7%. Took: 11.27s\n",
      "Epoch: 638. T, V acc: 99.8%, 41.5%. Took: 11.26s\n",
      "Epoch: 639. T, V acc: 99.8%, 41.0%. Took: 11.24s\n",
      "Epoch: 640. T, V acc: 99.8%, 41.2%. Took: 11.27s\n",
      "Epoch: 641. T, V acc: 99.8%, 41.0%. Took: 11.29s\n",
      "Epoch: 642. T, V acc: 99.8%, 40.8%. Took: 11.23s\n",
      "Epoch: 643. T, V acc: 99.8%, 41.0%. Took: 11.21s\n",
      "Epoch: 644. T, V acc: 99.8%, 41.3%. Took: 11.24s\n",
      "Epoch: 645. T, V acc: 99.9%, 41.1%. Took: 11.23s\n",
      "Epoch: 646. T, V acc: 99.8%, 41.3%. Took: 11.25s\n",
      "Epoch: 647. T, V acc: 99.8%, 41.5%. Took: 11.26s\n",
      "Epoch: 648. T, V acc: 99.8%, 41.6%. Took: 11.23s\n",
      "Epoch: 649. T, V acc: 99.8%, 41.3%. Took: 11.23s\n",
      "Epoch: 650. T, V acc: 99.8%, 40.5%. Took: 11.26s\n",
      "Epoch: 651. T, V acc: 99.8%, 41.2%. Took: 11.25s\n",
      "Epoch: 652. T, V acc: 99.8%, 41.3%. Took: 11.26s\n",
      "Epoch: 653. T, V acc: 99.8%, 41.2%. Took: 11.29s\n",
      "Epoch: 654. T, V acc: 99.8%, 41.1%. Took: 11.24s\n",
      "Epoch: 655. T, V acc: 99.8%, 41.5%. Took: 11.27s\n",
      "Epoch: 656. T, V acc: 99.8%, 41.1%. Took: 11.24s\n",
      "Epoch: 657. T, V acc: 99.8%, 41.1%. Took: 11.29s\n",
      "Epoch: 658. T, V acc: 99.8%, 41.5%. Took: 11.28s\n",
      "Epoch: 659. T, V acc: 99.8%, 40.9%. Took: 11.30s\n",
      "Epoch: 660. T, V acc: 99.8%, 40.5%. Took: 11.33s\n",
      "Epoch: 661. T, V acc: 99.8%, 40.6%. Took: 11.28s\n",
      "Epoch: 662. T, V acc: 99.8%, 40.8%. Took: 11.26s\n",
      "Epoch: 663. T, V acc: 99.8%, 40.8%. Took: 11.27s\n",
      "Epoch: 664. T, V acc: 99.9%, 40.9%. Took: 11.30s\n",
      "Epoch: 665. T, V acc: 99.8%, 41.0%. Took: 11.25s\n",
      "Epoch: 666. T, V acc: 99.8%, 40.8%. Took: 11.23s\n",
      "Epoch: 667. T, V acc: 99.8%, 40.3%. Took: 11.25s\n",
      "Epoch: 668. T, V acc: 99.8%, 41.2%. Took: 11.29s\n",
      "Epoch: 669. T, V acc: 99.8%, 41.3%. Took: 11.24s\n",
      "Epoch: 670. T, V acc: 99.8%, 41.4%. Took: 11.29s\n",
      "Epoch: 671. T, V acc: 99.8%, 41.6%. Took: 11.26s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 672. T, V acc: 99.8%, 41.4%. Took: 11.26s\n",
      "Epoch: 673. T, V acc: 99.8%, 41.5%. Took: 11.26s\n",
      "Epoch: 674. T, V acc: 99.8%, 40.8%. Took: 11.32s\n",
      "Epoch: 675. T, V acc: 99.8%, 41.5%. Took: 11.32s\n",
      "Epoch: 676. T, V acc: 99.8%, 41.1%. Took: 11.29s\n",
      "Epoch: 677. T, V acc: 99.8%, 40.5%. Took: 11.27s\n",
      "Epoch: 678. T, V acc: 99.8%, 40.5%. Took: 11.32s\n",
      "Epoch: 679. T, V acc: 99.8%, 41.3%. Took: 11.29s\n",
      "Epoch: 680. T, V acc: 99.8%, 41.1%. Took: 11.28s\n",
      "Epoch: 681. T, V acc: 99.8%, 41.6%. Took: 11.32s\n",
      "Epoch: 682. T, V acc: 99.8%, 40.6%. Took: 11.29s\n",
      "Epoch: 683. T, V acc: 99.8%, 40.7%. Took: 11.30s\n",
      "Epoch: 684. T, V acc: 99.8%, 41.1%. Took: 11.29s\n",
      "Epoch: 685. T, V acc: 99.8%, 40.3%. Took: 11.26s\n",
      "Epoch: 686. T, V acc: 99.8%, 41.0%. Took: 11.22s\n",
      "Epoch: 687. T, V acc: 99.8%, 41.4%. Took: 11.22s\n",
      "Epoch: 688. T, V acc: 99.8%, 41.0%. Took: 11.24s\n",
      "Epoch: 689. T, V acc: 99.8%, 41.4%. Took: 11.22s\n",
      "Epoch: 690. T, V acc: 99.8%, 41.3%. Took: 11.21s\n",
      "Epoch: 691. T, V acc: 99.8%, 41.1%. Took: 11.17s\n",
      "Epoch: 692. T, V acc: 99.8%, 41.5%. Took: 11.17s\n",
      "Epoch: 693. T, V acc: 99.8%, 41.2%. Took: 11.21s\n",
      "Epoch: 694. T, V acc: 99.8%, 40.5%. Took: 11.23s\n",
      "Epoch: 695. T, V acc: 99.8%, 41.2%. Took: 11.28s\n",
      "Epoch: 696. T, V acc: 99.8%, 41.2%. Took: 11.38s\n",
      "Epoch: 697. T, V acc: 99.8%, 41.0%. Took: 11.34s\n",
      "Epoch: 698. T, V acc: 99.8%, 40.9%. Took: 11.30s\n",
      "Epoch: 699. T, V acc: 99.8%, 40.4%. Took: 11.34s\n",
      "Epoch: 700. T, V acc: 99.8%, 40.5%. Took: 11.32s\n",
      "Epoch: 701. T, V acc: 99.8%, 40.8%. Took: 11.34s\n",
      "Epoch: 702. T, V acc: 99.9%, 40.7%. Took: 11.31s\n",
      "Epoch: 703. T, V acc: 99.8%, 41.2%. Took: 11.23s\n",
      "Epoch: 704. T, V acc: 99.8%, 40.9%. Took: 11.27s\n",
      "Epoch: 705. T, V acc: 99.8%, 41.3%. Took: 11.28s\n",
      "Epoch: 706. T, V acc: 99.8%, 40.9%. Took: 11.22s\n",
      "Epoch: 707. T, V acc: 99.8%, 40.9%. Took: 11.22s\n",
      "Epoch: 708. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 709. T, V acc: 99.8%, 40.9%. Took: 11.31s\n",
      "Epoch: 710. T, V acc: 99.8%, 40.6%. Took: 11.30s\n",
      "Epoch: 711. T, V acc: 99.9%, 40.2%. Took: 11.34s\n",
      "Epoch: 712. T, V acc: 99.8%, 40.8%. Took: 11.29s\n",
      "Epoch: 713. T, V acc: 99.8%, 41.0%. Took: 11.33s\n",
      "Epoch: 714. T, V acc: 99.8%, 40.9%. Took: 11.33s\n",
      "Epoch: 715. T, V acc: 99.8%, 41.3%. Took: 11.32s\n",
      "Epoch: 716. T, V acc: 99.8%, 41.2%. Took: 11.34s\n",
      "Epoch: 717. T, V acc: 99.8%, 40.9%. Took: 11.32s\n",
      "Epoch: 718. T, V acc: 99.8%, 40.8%. Took: 11.31s\n",
      "Epoch: 719. T, V acc: 99.8%, 40.5%. Took: 11.32s\n",
      "Epoch: 720. T, V acc: 99.8%, 40.9%. Took: 11.26s\n",
      "Epoch: 721. T, V acc: 99.8%, 40.7%. Took: 11.33s\n",
      "Epoch: 722. T, V acc: 99.8%, 40.6%. Took: 11.31s\n",
      "Epoch: 723. T, V acc: 99.8%, 40.5%. Took: 11.26s\n",
      "Epoch: 724. T, V acc: 99.8%, 40.2%. Took: 11.23s\n",
      "Epoch: 725. T, V acc: 99.8%, 39.9%. Took: 11.22s\n",
      "Epoch: 726. T, V acc: 99.8%, 40.0%. Took: 11.23s\n",
      "Epoch: 727. T, V acc: 99.8%, 40.2%. Took: 11.24s\n",
      "Epoch: 728. T, V acc: 99.9%, 40.3%. Took: 11.23s\n",
      "Epoch: 729. T, V acc: 99.8%, 40.4%. Took: 11.25s\n",
      "Epoch: 730. T, V acc: 99.8%, 40.2%. Took: 11.20s\n",
      "Epoch: 731. T, V acc: 99.8%, 40.3%. Took: 11.24s\n",
      "Epoch: 732. T, V acc: 99.9%, 40.6%. Took: 11.24s\n",
      "Epoch: 733. T, V acc: 99.9%, 40.1%. Took: 11.32s\n",
      "Epoch: 734. T, V acc: 99.8%, 40.3%. Took: 11.30s\n",
      "Epoch: 735. T, V acc: 99.9%, 40.2%. Took: 11.25s\n",
      "Epoch: 736. T, V acc: 99.9%, 40.3%. Took: 11.32s\n",
      "Epoch: 737. T, V acc: 99.9%, 40.2%. Took: 11.27s\n",
      "Epoch: 738. T, V acc: 99.9%, 40.4%. Took: 11.27s\n",
      "Epoch: 739. T, V acc: 99.8%, 40.6%. Took: 11.24s\n",
      "Epoch: 740. T, V acc: 99.9%, 40.6%. Took: 11.23s\n",
      "Epoch: 741. T, V acc: 99.9%, 40.3%. Took: 11.28s\n",
      "Epoch: 742. T, V acc: 99.8%, 40.4%. Took: 11.25s\n",
      "Epoch: 743. T, V acc: 99.9%, 40.6%. Took: 11.20s\n",
      "Epoch: 744. T, V acc: 99.8%, 40.5%. Took: 11.21s\n",
      "Epoch: 745. T, V acc: 99.8%, 39.6%. Took: 11.22s\n",
      "Epoch: 746. T, V acc: 99.8%, 40.1%. Took: 11.24s\n",
      "Epoch: 747. T, V acc: 99.8%, 40.3%. Took: 11.22s\n",
      "Epoch: 748. T, V acc: 99.8%, 40.0%. Took: 11.22s\n",
      "Epoch: 749. T, V acc: 99.8%, 39.7%. Took: 11.12s\n",
      "Epoch: 750. T, V acc: 99.8%, 40.2%. Took: 11.09s\n",
      "Epoch: 751. T, V acc: 99.8%, 40.1%. Took: 11.08s\n",
      "Epoch: 752. T, V acc: 99.8%, 40.2%. Took: 11.08s\n",
      "Epoch: 753. T, V acc: 99.8%, 40.1%. Took: 11.11s\n",
      "Epoch: 754. T, V acc: 99.8%, 40.3%. Took: 11.23s\n",
      "Epoch: 755. T, V acc: 99.8%, 40.1%. Took: 11.26s\n",
      "Epoch: 756. T, V acc: 99.8%, 40.8%. Took: 11.26s\n",
      "Epoch: 757. T, V acc: 99.8%, 40.3%. Took: 11.27s\n",
      "Epoch: 758. T, V acc: 99.8%, 40.5%. Took: 11.30s\n",
      "Epoch: 759. T, V acc: 99.8%, 40.7%. Took: 11.31s\n",
      "Epoch: 760. T, V acc: 99.8%, 40.9%. Took: 11.26s\n",
      "Epoch: 761. T, V acc: 99.8%, 40.9%. Took: 11.31s\n",
      "Epoch: 762. T, V acc: 99.8%, 40.6%. Took: 11.36s\n",
      "Epoch: 763. T, V acc: 99.8%, 40.4%. Took: 11.34s\n",
      "Epoch: 764. T, V acc: 99.8%, 40.6%. Took: 11.32s\n",
      "Epoch: 765. T, V acc: 99.8%, 40.8%. Took: 11.33s\n",
      "Epoch: 766. T, V acc: 99.9%, 40.7%. Took: 11.25s\n",
      "Epoch: 767. T, V acc: 99.8%, 40.8%. Took: 11.24s\n",
      "Epoch: 768. T, V acc: 99.8%, 40.9%. Took: 11.24s\n",
      "Epoch: 769. T, V acc: 99.8%, 41.0%. Took: 11.24s\n",
      "Epoch: 770. T, V acc: 99.8%, 40.8%. Took: 11.31s\n",
      "Epoch: 771. T, V acc: 99.8%, 40.8%. Took: 11.30s\n",
      "Epoch: 772. T, V acc: 99.8%, 41.0%. Took: 11.24s\n",
      "Epoch: 773. T, V acc: 99.8%, 40.0%. Took: 11.20s\n",
      "Epoch: 774. T, V acc: 99.8%, 39.7%. Took: 11.23s\n",
      "Epoch: 775. T, V acc: 99.8%, 40.2%. Took: 11.28s\n",
      "Epoch: 776. T, V acc: 99.8%, 41.4%. Took: 11.22s\n",
      "Epoch: 777. T, V acc: 99.8%, 40.8%. Took: 11.26s\n",
      "Epoch: 778. T, V acc: 99.8%, 40.4%. Took: 11.21s\n",
      "Epoch: 779. T, V acc: 99.8%, 40.1%. Took: 11.20s\n",
      "Epoch: 780. T, V acc: 99.8%, 40.0%. Took: 11.20s\n",
      "Epoch: 781. T, V acc: 99.8%, 39.4%. Took: 11.19s\n",
      "Epoch: 782. T, V acc: 99.8%, 39.9%. Took: 11.12s\n",
      "Epoch: 783. T, V acc: 99.8%, 40.3%. Took: 11.09s\n",
      "Epoch: 784. T, V acc: 99.8%, 39.8%. Took: 11.09s\n",
      "Epoch: 785. T, V acc: 99.8%, 39.4%. Took: 11.07s\n",
      "Epoch: 786. T, V acc: 99.8%, 39.8%. Took: 11.09s\n",
      "Epoch: 787. T, V acc: 99.8%, 39.7%. Took: 11.10s\n",
      "Epoch: 788. T, V acc: 99.8%, 39.7%. Took: 11.07s\n",
      "Epoch: 789. T, V acc: 99.8%, 39.8%. Took: 11.09s\n",
      "Epoch: 790. T, V acc: 99.8%, 40.1%. Took: 11.06s\n",
      "Epoch: 791. T, V acc: 99.8%, 39.6%. Took: 11.29s\n",
      "Epoch: 792. T, V acc: 99.8%, 39.8%. Took: 11.27s\n",
      "Epoch: 793. T, V acc: 99.8%, 39.6%. Took: 11.31s\n",
      "Epoch: 794. T, V acc: 99.8%, 39.8%. Took: 11.33s\n",
      "Epoch: 795. T, V acc: 99.8%, 40.0%. Took: 11.28s\n",
      "Epoch: 796. T, V acc: 99.8%, 40.0%. Took: 11.23s\n",
      "Epoch: 797. T, V acc: 99.8%, 39.9%. Took: 11.34s\n",
      "Epoch: 798. T, V acc: 99.8%, 40.1%. Took: 11.29s\n",
      "Epoch: 799. T, V acc: 99.8%, 40.0%. Took: 11.33s\n",
      "Epoch: 800. T, V acc: 99.8%, 40.0%. Took: 11.31s\n",
      "Epoch: 801. T, V acc: 99.8%, 39.9%. Took: 11.32s\n",
      "Epoch: 802. T, V acc: 99.8%, 39.8%. Took: 11.33s\n",
      "Epoch: 803. T, V acc: 99.8%, 39.9%. Took: 11.35s\n",
      "Epoch: 804. T, V acc: 99.8%, 39.8%. Took: 11.26s\n",
      "Epoch: 805. T, V acc: 99.8%, 40.5%. Took: 11.29s\n",
      "Epoch: 806. T, V acc: 99.8%, 40.6%. Took: 11.31s\n",
      "Epoch: 807. T, V acc: 99.8%, 40.5%. Took: 11.28s\n",
      "Epoch: 808. T, V acc: 99.8%, 40.6%. Took: 11.28s\n",
      "Epoch: 809. T, V acc: 99.8%, 40.7%. Took: 11.31s\n",
      "Epoch: 810. T, V acc: 99.8%, 40.6%. Took: 11.26s\n",
      "Epoch: 811. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 812. T, V acc: 99.8%, 40.8%. Took: 11.28s\n",
      "Epoch: 813. T, V acc: 99.8%, 40.7%. Took: 11.26s\n",
      "Epoch: 814. T, V acc: 99.9%, 40.6%. Took: 11.29s\n",
      "Epoch: 815. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 816. T, V acc: 99.8%, 40.6%. Took: 11.29s\n",
      "Epoch: 817. T, V acc: 99.8%, 40.5%. Took: 11.27s\n",
      "Epoch: 818. T, V acc: 99.8%, 40.7%. Took: 11.29s\n",
      "Epoch: 819. T, V acc: 99.8%, 40.3%. Took: 11.31s\n",
      "Epoch: 820. T, V acc: 99.8%, 40.5%. Took: 11.31s\n",
      "Epoch: 821. T, V acc: 99.8%, 40.3%. Took: 11.35s\n",
      "Epoch: 822. T, V acc: 99.9%, 40.3%. Took: 11.30s\n",
      "Epoch: 823. T, V acc: 99.8%, 40.2%. Took: 11.29s\n",
      "Epoch: 824. T, V acc: 99.9%, 40.1%. Took: 11.32s\n",
      "Epoch: 825. T, V acc: 99.8%, 40.0%. Took: 11.25s\n",
      "Epoch: 826. T, V acc: 99.8%, 40.3%. Took: 11.28s\n",
      "Epoch: 827. T, V acc: 99.8%, 40.7%. Took: 11.24s\n",
      "Epoch: 828. T, V acc: 99.8%, 40.2%. Took: 11.23s\n",
      "Epoch: 829. T, V acc: 99.8%, 40.6%. Took: 11.33s\n",
      "Epoch: 830. T, V acc: 99.8%, 40.8%. Took: 11.36s\n",
      "Epoch: 831. T, V acc: 99.8%, 40.2%. Took: 11.38s\n",
      "Epoch: 832. T, V acc: 99.8%, 40.5%. Took: 11.27s\n",
      "Epoch: 833. T, V acc: 99.8%, 40.7%. Took: 11.29s\n",
      "Epoch: 834. T, V acc: 99.8%, 40.8%. Took: 11.32s\n",
      "Epoch: 835. T, V acc: 99.8%, 40.9%. Took: 11.29s\n",
      "Epoch: 836. T, V acc: 99.9%, 40.9%. Took: 11.27s\n",
      "Epoch: 837. T, V acc: 99.8%, 41.0%. Took: 11.26s\n",
      "Epoch: 838. T, V acc: 99.8%, 40.3%. Took: 11.24s\n",
      "Epoch: 839. T, V acc: 99.8%, 40.8%. Took: 11.29s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 840. T, V acc: 99.8%, 40.4%. Took: 11.29s\n",
      "Epoch: 841. T, V acc: 99.8%, 40.4%. Took: 11.28s\n",
      "Epoch: 842. T, V acc: 99.8%, 40.4%. Took: 11.21s\n",
      "Epoch: 843. T, V acc: 99.8%, 40.7%. Took: 11.22s\n",
      "Epoch: 844. T, V acc: 99.8%, 40.8%. Took: 11.24s\n",
      "Epoch: 845. T, V acc: 99.8%, 40.3%. Took: 11.25s\n",
      "Epoch: 846. T, V acc: 99.8%, 40.4%. Took: 11.22s\n",
      "Epoch: 847. T, V acc: 99.8%, 40.1%. Took: 11.24s\n",
      "Epoch: 848. T, V acc: 99.8%, 40.4%. Took: 11.16s\n",
      "Epoch: 849. T, V acc: 99.8%, 40.7%. Took: 11.17s\n",
      "Epoch: 850. T, V acc: 99.8%, 40.2%. Took: 11.20s\n",
      "Epoch: 851. T, V acc: 99.8%, 40.7%. Took: 11.19s\n",
      "Epoch: 852. T, V acc: 99.8%, 40.9%. Took: 11.20s\n",
      "Epoch: 853. T, V acc: 99.8%, 40.4%. Took: 11.21s\n",
      "Epoch: 854. T, V acc: 99.8%, 40.3%. Took: 11.23s\n",
      "Epoch: 855. T, V acc: 99.8%, 40.0%. Took: 11.23s\n",
      "Epoch: 856. T, V acc: 99.8%, 40.3%. Took: 11.23s\n",
      "Epoch: 857. T, V acc: 99.8%, 40.3%. Took: 11.20s\n",
      "Epoch: 858. T, V acc: 99.8%, 40.4%. Took: 11.22s\n",
      "Epoch: 859. T, V acc: 99.8%, 40.4%. Took: 11.20s\n",
      "Epoch: 860. T, V acc: 99.8%, 40.5%. Took: 11.21s\n",
      "Epoch: 861. T, V acc: 99.8%, 40.5%. Took: 11.15s\n",
      "Epoch: 862. T, V acc: 99.8%, 40.6%. Took: 11.26s\n",
      "Epoch: 863. T, V acc: 99.8%, 40.2%. Took: 11.23s\n",
      "Epoch: 864. T, V acc: 99.8%, 40.6%. Took: 11.27s\n",
      "Epoch: 865. T, V acc: 99.8%, 40.5%. Took: 11.25s\n",
      "Epoch: 866. T, V acc: 99.8%, 40.5%. Took: 11.23s\n",
      "Epoch: 867. T, V acc: 99.8%, 40.5%. Took: 11.18s\n",
      "Epoch: 868. T, V acc: 99.8%, 40.3%. Took: 11.14s\n",
      "Epoch: 869. T, V acc: 99.8%, 40.5%. Took: 11.17s\n",
      "Epoch: 870. T, V acc: 99.8%, 39.8%. Took: 11.17s\n",
      "Epoch: 871. T, V acc: 99.8%, 40.0%. Took: 11.19s\n",
      "Epoch: 872. T, V acc: 99.8%, 40.4%. Took: 11.16s\n",
      "Epoch: 873. T, V acc: 99.8%, 40.4%. Took: 11.18s\n",
      "Epoch: 874. T, V acc: 99.8%, 40.5%. Took: 11.21s\n",
      "Epoch: 875. T, V acc: 99.8%, 40.6%. Took: 11.24s\n",
      "Epoch: 876. T, V acc: 99.8%, 40.6%. Took: 11.22s\n",
      "Epoch: 877. T, V acc: 99.8%, 40.7%. Took: 11.21s\n",
      "Epoch: 878. T, V acc: 99.8%, 40.7%. Took: 11.24s\n",
      "Epoch: 879. T, V acc: 99.8%, 40.7%. Took: 11.31s\n",
      "Epoch: 880. T, V acc: 99.8%, 40.5%. Took: 11.25s\n",
      "Epoch: 881. T, V acc: 99.8%, 40.6%. Took: 11.23s\n",
      "Epoch: 882. T, V acc: 99.8%, 40.5%. Took: 11.28s\n",
      "Epoch: 883. T, V acc: 99.8%, 40.7%. Took: 11.36s\n",
      "Epoch: 884. T, V acc: 99.8%, 40.5%. Took: 11.36s\n",
      "Epoch: 885. T, V acc: 99.8%, 40.5%. Took: 11.31s\n",
      "Epoch: 886. T, V acc: 99.8%, 40.2%. Took: 11.35s\n",
      "Epoch: 887. T, V acc: 99.8%, 40.4%. Took: 11.32s\n",
      "Epoch: 888. T, V acc: 99.8%, 40.5%. Took: 11.28s\n",
      "Epoch: 889. T, V acc: 99.8%, 40.5%. Took: 11.30s\n",
      "Epoch: 890. T, V acc: 99.8%, 40.3%. Took: 11.24s\n",
      "Epoch: 891. T, V acc: 99.8%, 40.4%. Took: 11.23s\n",
      "Epoch: 892. T, V acc: 99.8%, 40.6%. Took: 11.33s\n",
      "Epoch: 893. T, V acc: 99.8%, 40.4%. Took: 11.24s\n",
      "Epoch: 894. T, V acc: 99.8%, 40.9%. Took: 11.14s\n",
      "Epoch: 895. T, V acc: 99.8%, 40.5%. Took: 11.09s\n",
      "Epoch: 896. T, V acc: 99.8%, 40.6%. Took: 11.11s\n",
      "Epoch: 897. T, V acc: 99.8%, 40.4%. Took: 11.31s\n",
      "Epoch: 898. T, V acc: 99.8%, 40.8%. Took: 11.31s\n",
      "Epoch: 899. T, V acc: 99.8%, 40.1%. Took: 11.29s\n",
      "Epoch: 900. T, V acc: 99.8%, 40.7%. Took: 11.30s\n",
      "Epoch: 901. T, V acc: 99.8%, 40.3%. Took: 11.30s\n",
      "Epoch: 902. T, V acc: 99.8%, 39.8%. Took: 11.29s\n",
      "Epoch: 903. T, V acc: 99.8%, 40.4%. Took: 11.19s\n",
      "Epoch: 904. T, V acc: 99.8%, 40.5%. Took: 11.22s\n",
      "Epoch: 905. T, V acc: 99.8%, 40.6%. Took: 11.21s\n",
      "Epoch: 906. T, V acc: 99.8%, 40.5%. Took: 11.23s\n",
      "Epoch: 907. T, V acc: 99.8%, 40.2%. Took: 11.22s\n",
      "Epoch: 908. T, V acc: 99.8%, 40.2%. Took: 11.25s\n",
      "Epoch: 909. T, V acc: 99.8%, 40.4%. Took: 11.27s\n",
      "Epoch: 910. T, V acc: 99.8%, 40.3%. Took: 11.21s\n",
      "Epoch: 911. T, V acc: 99.8%, 40.5%. Took: 11.23s\n",
      "Epoch: 912. T, V acc: 99.8%, 40.6%. Took: 11.31s\n",
      "Epoch: 913. T, V acc: 99.8%, 40.6%. Took: 11.30s\n",
      "Epoch: 914. T, V acc: 99.8%, 40.6%. Took: 11.27s\n",
      "Epoch: 915. T, V acc: 99.8%, 40.7%. Took: 11.30s\n",
      "Epoch: 916. T, V acc: 99.8%, 40.5%. Took: 11.28s\n",
      "Epoch: 917. T, V acc: 99.8%, 40.5%. Took: 11.26s\n",
      "Epoch: 918. T, V acc: 99.8%, 40.1%. Took: 11.29s\n",
      "Epoch: 919. T, V acc: 99.8%, 40.3%. Took: 11.25s\n",
      "Epoch: 920. T, V acc: 99.8%, 40.3%. Took: 11.25s\n",
      "Epoch: 921. T, V acc: 99.8%, 40.4%. Took: 11.24s\n",
      "Epoch: 922. T, V acc: 99.8%, 40.5%. Took: 11.26s\n",
      "Epoch: 923. T, V acc: 99.8%, 40.3%. Took: 11.26s\n",
      "Epoch: 924. T, V acc: 99.8%, 39.9%. Took: 11.27s\n",
      "Epoch: 925. T, V acc: 99.8%, 39.7%. Took: 11.30s\n",
      "Epoch: 926. T, V acc: 99.8%, 40.0%. Took: 11.25s\n",
      "Epoch: 927. T, V acc: 99.8%, 39.8%. Took: 11.24s\n",
      "Epoch: 928. T, V acc: 99.8%, 40.0%. Took: 11.31s\n",
      "Epoch: 929. T, V acc: 99.8%, 40.0%. Took: 11.27s\n",
      "Epoch: 930. T, V acc: 99.8%, 40.2%. Took: 11.20s\n",
      "Epoch: 931. T, V acc: 99.8%, 40.5%. Took: 11.11s\n",
      "Epoch: 932. T, V acc: 99.8%, 40.5%. Took: 11.10s\n",
      "Epoch: 933. T, V acc: 99.8%, 40.6%. Took: 11.09s\n",
      "Epoch: 934. T, V acc: 99.8%, 39.8%. Took: 11.27s\n",
      "Epoch: 935. T, V acc: 99.8%, 40.6%. Took: 11.22s\n",
      "Epoch: 936. T, V acc: 99.8%, 40.4%. Took: 11.25s\n",
      "Epoch: 937. T, V acc: 99.8%, 40.5%. Took: 11.26s\n",
      "Epoch: 938. T, V acc: 99.8%, 40.9%. Took: 11.28s\n",
      "Epoch: 939. T, V acc: 99.8%, 41.0%. Took: 11.29s\n",
      "Epoch: 940. T, V acc: 99.8%, 40.8%. Took: 11.26s\n",
      "Epoch: 941. T, V acc: 99.8%, 40.6%. Took: 11.30s\n",
      "Epoch: 942. T, V acc: 99.8%, 40.6%. Took: 11.29s\n",
      "Epoch: 943. T, V acc: 99.8%, 40.7%. Took: 11.27s\n",
      "Epoch: 944. T, V acc: 99.8%, 40.6%. Took: 11.26s\n",
      "Epoch: 945. T, V acc: 99.8%, 40.6%. Took: 11.28s\n",
      "Epoch: 946. T, V acc: 99.8%, 40.8%. Took: 11.28s\n",
      "Epoch: 947. T, V acc: 99.8%, 40.8%. Took: 11.30s\n",
      "Epoch: 948. T, V acc: 99.8%, 40.9%. Took: 11.26s\n",
      "Epoch: 949. T, V acc: 99.8%, 40.9%. Took: 11.30s\n",
      "Epoch: 950. T, V acc: 99.8%, 40.9%. Took: 11.28s\n",
      "Epoch: 951. T, V acc: 99.8%, 40.8%. Took: 11.30s\n",
      "Epoch: 952. T, V acc: 99.8%, 40.8%. Took: 11.30s\n",
      "Epoch: 953. T, V acc: 99.8%, 41.1%. Took: 11.24s\n",
      "Epoch: 954. T, V acc: 99.9%, 40.6%. Took: 11.26s\n",
      "Epoch: 955. T, V acc: 99.8%, 40.8%. Took: 11.23s\n",
      "Epoch: 956. T, V acc: 99.8%, 41.3%. Took: 11.23s\n",
      "Epoch: 957. T, V acc: 99.8%, 41.2%. Took: 11.25s\n",
      "Epoch: 958. T, V acc: 99.8%, 41.4%. Took: 11.27s\n",
      "Epoch: 959. T, V acc: 99.8%, 41.1%. Took: 11.23s\n",
      "Epoch: 960. T, V acc: 99.8%, 41.2%. Took: 11.24s\n",
      "Epoch: 961. T, V acc: 99.8%, 41.1%. Took: 11.25s\n",
      "Epoch: 962. T, V acc: 99.8%, 41.2%. Took: 11.24s\n",
      "Epoch: 963. T, V acc: 99.8%, 41.4%. Took: 11.26s\n",
      "Epoch: 964. T, V acc: 99.8%, 41.0%. Took: 11.28s\n",
      "Epoch: 965. T, V acc: 99.8%, 40.9%. Took: 11.23s\n",
      "Epoch: 966. T, V acc: 99.8%, 41.1%. Took: 11.28s\n",
      "Epoch: 967. T, V acc: 99.8%, 41.3%. Took: 11.27s\n",
      "Epoch: 968. T, V acc: 99.9%, 41.4%. Took: 11.28s\n",
      "Epoch: 969. T, V acc: 99.8%, 41.8%. Took: 11.24s\n",
      "Epoch: 970. T, V acc: 99.8%, 41.2%. Took: 11.22s\n",
      "Epoch: 971. T, V acc: 99.8%, 41.4%. Took: 11.27s\n",
      "Epoch: 972. T, V acc: 99.8%, 41.8%. Took: 11.28s\n",
      "Epoch: 973. T, V acc: 99.9%, 41.3%. Took: 11.26s\n",
      "Epoch: 974. T, V acc: 99.8%, 41.2%. Took: 11.23s\n",
      "Epoch: 975. T, V acc: 99.8%, 41.1%. Took: 11.25s\n",
      "Epoch: 976. T, V acc: 99.8%, 41.2%. Took: 11.25s\n",
      "Epoch: 977. T, V acc: 99.8%, 40.0%. Took: 11.13s\n",
      "Epoch: 978. T, V acc: 99.8%, 40.8%. Took: 11.07s\n",
      "Epoch: 979. T, V acc: 99.8%, 40.4%. Took: 11.10s\n",
      "Epoch: 980. T, V acc: 99.8%, 40.6%. Took: 11.10s\n",
      "Epoch: 981. T, V acc: 99.8%, 40.5%. Took: 11.09s\n",
      "Epoch: 982. T, V acc: 99.8%, 40.3%. Took: 11.09s\n",
      "Epoch: 983. T, V acc: 99.8%, 40.6%. Took: 11.09s\n",
      "Epoch: 984. T, V acc: 99.8%, 38.8%. Took: 11.16s\n",
      "Epoch: 985. T, V acc: 99.8%, 39.6%. Took: 11.20s\n",
      "Epoch: 986. T, V acc: 99.8%, 40.0%. Took: 11.18s\n",
      "Epoch: 987. T, V acc: 99.8%, 40.0%. Took: 11.24s\n",
      "Epoch: 988. T, V acc: 99.8%, 40.2%. Took: 11.23s\n",
      "Epoch: 989. T, V acc: 99.8%, 39.4%. Took: 11.24s\n",
      "Epoch: 990. T, V acc: 99.8%, 40.1%. Took: 11.19s\n",
      "Epoch: 991. T, V acc: 99.8%, 40.0%. Took: 11.17s\n",
      "Epoch: 992. T, V acc: 99.8%, 39.7%. Took: 11.28s\n",
      "Epoch: 993. T, V acc: 99.8%, 39.9%. Took: 11.30s\n",
      "Epoch: 994. T, V acc: 99.8%, 39.9%. Took: 11.26s\n",
      "Epoch: 995. T, V acc: 99.8%, 40.0%. Took: 11.28s\n",
      "Epoch: 996. T, V acc: 99.8%, 40.0%. Took: 11.23s\n",
      "Epoch: 997. T, V acc: 99.8%, 39.6%. Took: 11.27s\n",
      "Epoch: 998. T, V acc: 99.8%, 40.2%. Took: 11.30s\n",
      "Epoch: 999. T, V acc: 99.8%, 40.5%. Took: 11.30s\n",
      "Epoch: 1000. T, V acc: 99.8%, 40.5%. Took: 11.28s\n",
      "Epoch: 1001. T, V acc: 99.8%, 40.6%. Took: 11.27s\n",
      "Epoch: 1002. T, V acc: 99.8%, 40.6%. Took: 11.32s\n",
      "Epoch: 1003. T, V acc: 99.8%, 40.6%. Took: 11.31s\n",
      "Epoch: 1004. T, V acc: 99.8%, 40.6%. Took: 11.33s\n",
      "Epoch: 1005. T, V acc: 99.8%, 40.6%. Took: 11.32s\n",
      "Epoch: 1006. T, V acc: 99.8%, 40.5%. Took: 11.30s\n",
      "Epoch: 1007. T, V acc: 99.8%, 40.8%. Took: 11.34s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1008. T, V acc: 99.8%, 40.8%. Took: 11.22s\n",
      "Epoch: 1009. T, V acc: 99.8%, 40.6%. Took: 11.12s\n",
      "Epoch: 1010. T, V acc: 99.8%, 40.6%. Took: 11.29s\n",
      "Epoch: 1011. T, V acc: 99.8%, 40.5%. Took: 11.33s\n",
      "Epoch: 1012. T, V acc: 99.8%, 40.6%. Took: 11.35s\n",
      "Epoch: 1013. T, V acc: 99.8%, 40.5%. Took: 11.30s\n",
      "Epoch: 1014. T, V acc: 99.8%, 40.5%. Took: 11.31s\n",
      "Epoch: 1015. T, V acc: 99.9%, 40.2%. Took: 11.26s\n",
      "Epoch: 1016. T, V acc: 99.8%, 40.2%. Took: 11.28s\n",
      "Epoch: 1017. T, V acc: 99.8%, 40.5%. Took: 11.22s\n",
      "Epoch: 1018. T, V acc: 99.8%, 40.9%. Took: 11.22s\n",
      "Epoch: 1019. T, V acc: 99.8%, 40.9%. Took: 11.21s\n",
      "Epoch: 1020. T, V acc: 99.8%, 40.9%. Took: 11.23s\n",
      "Epoch: 1021. T, V acc: 99.9%, 41.0%. Took: 11.19s\n",
      "Epoch: 1022. T, V acc: 99.8%, 41.4%. Took: 11.20s\n",
      "Epoch: 1023. T, V acc: 99.8%, 41.2%. Took: 11.18s\n",
      "Epoch: 1024. T, V acc: 99.8%, 41.0%. Took: 11.20s\n",
      "Epoch: 1025. T, V acc: 99.9%, 41.1%. Took: 11.23s\n",
      "Epoch: 1026. T, V acc: 99.9%, 41.1%. Took: 11.23s\n",
      "Epoch: 1027. T, V acc: 99.9%, 41.0%. Took: 11.23s\n",
      "Epoch: 1028. T, V acc: 99.9%, 41.1%. Took: 11.19s\n",
      "Epoch: 1029. T, V acc: 99.8%, 41.2%. Took: 11.22s\n",
      "Epoch: 1030. T, V acc: 99.8%, 40.5%. Took: 11.20s\n",
      "Epoch: 1031. T, V acc: 99.8%, 40.5%. Took: 11.23s\n",
      "Epoch: 1032. T, V acc: 99.8%, 41.5%. Took: 11.19s\n",
      "Epoch: 1033. T, V acc: 99.8%, 40.4%. Took: 11.19s\n",
      "Epoch: 1034. T, V acc: 99.8%, 40.6%. Took: 11.20s\n",
      "Epoch: 1035. T, V acc: 99.8%, 40.8%. Took: 11.21s\n",
      "Epoch: 1036. T, V acc: 99.8%, 41.0%. Took: 11.15s\n",
      "Epoch: 1037. T, V acc: 99.8%, 41.1%. Took: 11.22s\n",
      "Epoch: 1038. T, V acc: 99.8%, 40.9%. Took: 11.19s\n",
      "Epoch: 1039. T, V acc: 99.8%, 40.5%. Took: 11.22s\n",
      "Epoch: 1040. T, V acc: 99.9%, 41.1%. Took: 11.22s\n",
      "Epoch: 1041. T, V acc: 99.9%, 41.1%. Took: 11.22s\n",
      "Epoch: 1042. T, V acc: 99.8%, 41.2%. Took: 11.22s\n",
      "Epoch: 1043. T, V acc: 99.8%, 41.1%. Took: 11.19s\n",
      "Epoch: 1044. T, V acc: 99.8%, 40.6%. Took: 11.21s\n",
      "Epoch: 1045. T, V acc: 99.8%, 41.0%. Took: 11.23s\n",
      "Epoch: 1046. T, V acc: 99.8%, 41.0%. Took: 11.30s\n",
      "Epoch: 1047. T, V acc: 99.9%, 40.4%. Took: 11.24s\n",
      "Epoch: 1048. T, V acc: 99.9%, 40.7%. Took: 11.30s\n",
      "Epoch: 1049. T, V acc: 99.8%, 41.0%. Took: 11.28s\n",
      "Epoch: 1050. T, V acc: 99.8%, 40.9%. Took: 11.25s\n",
      "Epoch: 1051. T, V acc: 99.8%, 40.7%. Took: 11.27s\n",
      "Epoch: 1052. T, V acc: 99.9%, 41.2%. Took: 11.30s\n",
      "Epoch: 1053. T, V acc: 99.9%, 41.3%. Took: 11.28s\n",
      "Epoch: 1054. T, V acc: 99.8%, 42.0%. Took: 11.28s\n",
      "Epoch: 1055. T, V acc: 99.8%, 41.3%. Took: 11.34s\n",
      "Epoch: 1056. T, V acc: 99.8%, 41.2%. Took: 11.35s\n",
      "Epoch: 1057. T, V acc: 99.8%, 40.7%. Took: 11.38s\n",
      "Epoch: 1058. T, V acc: 99.8%, 41.0%. Took: 11.38s\n",
      "Epoch: 1059. T, V acc: 99.8%, 41.3%. Took: 11.33s\n",
      "Epoch: 1060. T, V acc: 99.8%, 41.0%. Took: 11.32s\n",
      "Epoch: 1061. T, V acc: 99.8%, 40.7%. Took: 11.31s\n",
      "Epoch: 1062. T, V acc: 99.8%, 40.9%. Took: 11.29s\n",
      "Epoch: 1063. T, V acc: 99.8%, 40.9%. Took: 11.25s\n",
      "Epoch: 1064. T, V acc: 99.8%, 41.2%. Took: 11.21s\n",
      "Epoch: 1065. T, V acc: 99.8%, 41.4%. Took: 11.30s\n",
      "Epoch: 1066. T, V acc: 99.8%, 41.3%. Took: 11.31s\n",
      "Epoch: 1067. T, V acc: 99.8%, 41.3%. Took: 11.26s\n",
      "Epoch: 1068. T, V acc: 99.8%, 41.0%. Took: 11.25s\n",
      "Epoch: 1069. T, V acc: 99.8%, 41.0%. Took: 11.28s\n",
      "Epoch: 1070. T, V acc: 99.8%, 41.3%. Took: 11.25s\n",
      "Epoch: 1071. T, V acc: 99.8%, 41.3%. Took: 11.23s\n",
      "Epoch: 1072. T, V acc: 99.8%, 40.9%. Took: 11.21s\n",
      "Epoch: 1073. T, V acc: 99.8%, 41.0%. Took: 11.24s\n",
      "Epoch: 1074. T, V acc: 99.9%, 41.1%. Took: 11.24s\n",
      "Epoch: 1075. T, V acc: 99.8%, 41.1%. Took: 11.27s\n",
      "Epoch: 1076. T, V acc: 99.8%, 41.2%. Took: 11.24s\n",
      "Epoch: 1077. T, V acc: 99.8%, 41.3%. Took: 11.26s\n",
      "Epoch: 1078. T, V acc: 99.8%, 41.3%. Took: 11.29s\n",
      "Epoch: 1079. T, V acc: 99.8%, 41.0%. Took: 11.28s\n",
      "Epoch: 1080. T, V acc: 99.9%, 41.6%. Took: 11.25s\n",
      "Epoch: 1081. T, V acc: 99.8%, 41.3%. Took: 11.23s\n",
      "Epoch: 1082. T, V acc: 99.8%, 41.2%. Took: 11.23s\n",
      "Epoch: 1083. T, V acc: 99.8%, 41.2%. Took: 11.12s\n",
      "Epoch: 1084. T, V acc: 99.8%, 41.2%. Took: 11.08s\n",
      "Epoch: 1085. T, V acc: 99.8%, 40.8%. Took: 11.15s\n",
      "Epoch: 1086. T, V acc: 99.8%, 40.8%. Took: 11.26s\n",
      "Epoch: 1087. T, V acc: 99.8%, 41.5%. Took: 11.24s\n",
      "Epoch: 1088. T, V acc: 99.8%, 41.6%. Took: 11.29s\n",
      "Epoch: 1089. T, V acc: 99.8%, 41.5%. Took: 11.29s\n",
      "Epoch: 1090. T, V acc: 99.8%, 40.6%. Took: 11.27s\n",
      "Epoch: 1091. T, V acc: 99.8%, 41.0%. Took: 11.23s\n",
      "Epoch: 1092. T, V acc: 99.8%, 40.6%. Took: 11.25s\n",
      "Epoch: 1093. T, V acc: 99.8%, 40.5%. Took: 11.24s\n",
      "Epoch: 1094. T, V acc: 99.8%, 40.8%. Took: 11.21s\n",
      "Epoch: 1095. T, V acc: 99.8%, 41.1%. Took: 11.29s\n",
      "Epoch: 1096. T, V acc: 99.8%, 40.9%. Took: 11.27s\n",
      "Epoch: 1097. T, V acc: 99.8%, 41.0%. Took: 11.26s\n",
      "Epoch: 1098. T, V acc: 99.8%, 41.1%. Took: 11.28s\n",
      "Epoch: 1099. T, V acc: 99.8%, 41.1%. Took: 11.23s\n",
      "Epoch: 1100. T, V acc: 99.8%, 40.8%. Took: 11.26s\n",
      "Epoch: 1101. T, V acc: 99.8%, 41.0%. Took: 11.31s\n",
      "Epoch: 1102. T, V acc: 99.8%, 41.3%. Took: 11.28s\n",
      "Epoch: 1103. T, V acc: 99.8%, 41.2%. Took: 11.27s\n",
      "Epoch: 1104. T, V acc: 99.8%, 40.4%. Took: 11.22s\n",
      "Epoch: 1105. T, V acc: 99.8%, 40.8%. Took: 11.26s\n",
      "Epoch: 1106. T, V acc: 99.8%, 40.5%. Took: 11.28s\n",
      "Epoch: 1107. T, V acc: 99.8%, 40.9%. Took: 11.25s\n",
      "Epoch: 1108. T, V acc: 99.8%, 41.1%. Took: 11.23s\n",
      "Epoch: 1109. T, V acc: 99.8%, 41.0%. Took: 11.25s\n",
      "Epoch: 1110. T, V acc: 99.8%, 41.0%. Took: 11.22s\n",
      "Epoch: 1111. T, V acc: 99.8%, 40.8%. Took: 11.21s\n",
      "Epoch: 1112. T, V acc: 99.8%, 41.0%. Took: 11.28s\n",
      "Epoch: 1113. T, V acc: 99.8%, 40.9%. Took: 11.25s\n",
      "Epoch: 1114. T, V acc: 99.8%, 41.1%. Took: 11.26s\n",
      "Epoch: 1115. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 1116. T, V acc: 99.8%, 40.6%. Took: 11.24s\n",
      "Epoch: 1117. T, V acc: 99.8%, 40.8%. Took: 11.22s\n",
      "Epoch: 1118. T, V acc: 99.8%, 40.6%. Took: 11.25s\n",
      "Epoch: 1119. T, V acc: 99.8%, 40.5%. Took: 11.28s\n",
      "Epoch: 1120. T, V acc: 99.9%, 41.2%. Took: 11.29s\n",
      "Epoch: 1121. T, V acc: 99.8%, 40.9%. Took: 11.29s\n",
      "Epoch: 1122. T, V acc: 99.8%, 41.0%. Took: 11.27s\n",
      "Epoch: 1123. T, V acc: 99.8%, 41.2%. Took: 11.25s\n",
      "Epoch: 1124. T, V acc: 99.8%, 41.3%. Took: 11.26s\n",
      "Epoch: 1125. T, V acc: 99.8%, 40.9%. Took: 11.22s\n",
      "Epoch: 1126. T, V acc: 99.8%, 40.3%. Took: 11.27s\n",
      "Epoch: 1127. T, V acc: 99.8%, 40.6%. Took: 11.29s\n",
      "Epoch: 1128. T, V acc: 99.8%, 40.6%. Took: 11.30s\n",
      "Epoch: 1129. T, V acc: 99.8%, 40.7%. Took: 11.24s\n",
      "Epoch: 1130. T, V acc: 99.8%, 41.1%. Took: 11.20s\n",
      "Epoch: 1131. T, V acc: 99.8%, 41.0%. Took: 11.22s\n",
      "Epoch: 1132. T, V acc: 99.8%, 41.1%. Took: 11.23s\n",
      "Epoch: 1133. T, V acc: 99.8%, 41.1%. Took: 11.23s\n",
      "Epoch: 1134. T, V acc: 99.8%, 41.2%. Took: 11.24s\n",
      "Epoch: 1135. T, V acc: 99.8%, 41.2%. Took: 11.22s\n",
      "Epoch: 1136. T, V acc: 99.8%, 41.0%. Took: 11.19s\n",
      "Epoch: 1137. T, V acc: 99.8%, 41.5%. Took: 11.22s\n",
      "Epoch: 1138. T, V acc: 99.9%, 41.3%. Took: 11.22s\n",
      "Epoch: 1139. T, V acc: 99.8%, 41.4%. Took: 11.21s\n",
      "Epoch: 1140. T, V acc: 99.8%, 41.3%. Took: 11.23s\n",
      "Epoch: 1141. T, V acc: 99.8%, 41.4%. Took: 11.27s\n",
      "Epoch: 1142. T, V acc: 99.8%, 41.5%. Took: 11.23s\n",
      "Epoch: 1143. T, V acc: 99.8%, 41.0%. Took: 11.22s\n",
      "Epoch: 1144. T, V acc: 99.8%, 41.1%. Took: 11.30s\n",
      "Epoch: 1145. T, V acc: 99.8%, 41.1%. Took: 11.25s\n",
      "Epoch: 1146. T, V acc: 99.8%, 41.1%. Took: 11.22s\n",
      "Epoch: 1147. T, V acc: 99.8%, 39.9%. Took: 11.21s\n",
      "Epoch: 1148. T, V acc: 99.8%, 40.8%. Took: 11.21s\n",
      "Epoch: 1149. T, V acc: 99.9%, 40.9%. Took: 11.25s\n",
      "Epoch: 1150. T, V acc: 99.9%, 41.0%. Took: 11.24s\n",
      "Epoch: 1151. T, V acc: 99.8%, 40.4%. Took: 11.21s\n",
      "Epoch: 1152. T, V acc: 99.8%, 40.7%. Took: 11.24s\n",
      "Epoch: 1153. T, V acc: 99.8%, 40.4%. Took: 11.19s\n",
      "Epoch: 1154. T, V acc: 99.9%, 41.3%. Took: 11.20s\n",
      "Epoch: 1155. T, V acc: 99.8%, 41.0%. Took: 11.21s\n",
      "Epoch: 1156. T, V acc: 99.8%, 41.2%. Took: 11.19s\n",
      "Epoch: 1157. T, V acc: 99.8%, 40.9%. Took: 11.21s\n",
      "Epoch: 1158. T, V acc: 99.9%, 41.1%. Took: 11.19s\n",
      "Epoch: 1159. T, V acc: 99.9%, 40.9%. Took: 11.20s\n",
      "Epoch: 1160. T, V acc: 99.8%, 40.8%. Took: 11.19s\n",
      "Epoch: 1161. T, V acc: 99.8%, 40.9%. Took: 11.24s\n",
      "Epoch: 1162. T, V acc: 99.9%, 41.2%. Took: 11.22s\n",
      "Epoch: 1163. T, V acc: 99.9%, 41.3%. Took: 11.19s\n",
      "Epoch: 1164. T, V acc: 99.8%, 41.4%. Took: 11.20s\n",
      "Epoch: 1165. T, V acc: 99.9%, 41.8%. Took: 11.24s\n",
      "Epoch: 1166. T, V acc: 99.9%, 41.8%. Took: 11.19s\n",
      "Epoch: 1167. T, V acc: 99.8%, 41.9%. Took: 11.18s\n",
      "Epoch: 1168. T, V acc: 99.8%, 40.7%. Took: 11.20s\n",
      "Epoch: 1169. T, V acc: 99.9%, 41.1%. Took: 11.21s\n",
      "Epoch: 1170. T, V acc: 99.8%, 41.1%. Took: 11.19s\n",
      "Epoch: 1171. T, V acc: 99.8%, 40.9%. Took: 11.22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1172. T, V acc: 99.9%, 40.8%. Took: 11.29s\n",
      "Epoch: 1173. T, V acc: 99.8%, 41.0%. Took: 11.29s\n",
      "Epoch: 1174. T, V acc: 99.8%, 41.4%. Took: 11.22s\n",
      "Epoch: 1175. T, V acc: 99.8%, 40.8%. Took: 11.26s\n",
      "Epoch: 1176. T, V acc: 99.9%, 41.0%. Took: 11.21s\n",
      "Epoch: 1177. T, V acc: 99.8%, 41.2%. Took: 11.18s\n",
      "Epoch: 1178. T, V acc: 99.9%, 41.1%. Took: 11.18s\n",
      "Epoch: 1179. T, V acc: 99.8%, 40.6%. Took: 11.18s\n",
      "Epoch: 1180. T, V acc: 99.8%, 41.0%. Took: 11.18s\n",
      "Epoch: 1181. T, V acc: 99.8%, 41.0%. Took: 11.19s\n",
      "Epoch: 1182. T, V acc: 99.8%, 41.1%. Took: 11.26s\n",
      "Epoch: 1183. T, V acc: 99.8%, 41.2%. Took: 11.24s\n",
      "Epoch: 1184. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 1185. T, V acc: 99.8%, 40.9%. Took: 11.37s\n",
      "Epoch: 1186. T, V acc: 99.8%, 41.0%. Took: 11.32s\n",
      "Epoch: 1187. T, V acc: 99.8%, 40.6%. Took: 11.27s\n",
      "Epoch: 1188. T, V acc: 99.9%, 41.4%. Took: 11.25s\n",
      "Epoch: 1189. T, V acc: 99.8%, 41.2%. Took: 11.23s\n",
      "Epoch: 1190. T, V acc: 99.8%, 41.4%. Took: 11.28s\n",
      "Epoch: 1191. T, V acc: 99.8%, 40.9%. Took: 11.25s\n",
      "Epoch: 1192. T, V acc: 99.8%, 41.0%. Took: 11.25s\n",
      "Epoch: 1193. T, V acc: 99.8%, 41.3%. Took: 11.26s\n",
      "Epoch: 1194. T, V acc: 99.8%, 41.1%. Took: 11.29s\n",
      "Epoch: 1195. T, V acc: 99.8%, 41.1%. Took: 11.28s\n",
      "Epoch: 1196. T, V acc: 99.8%, 41.1%. Took: 11.30s\n",
      "Epoch: 1197. T, V acc: 99.8%, 41.1%. Took: 11.31s\n",
      "Epoch: 1198. T, V acc: 99.8%, 41.0%. Took: 11.29s\n",
      "Epoch: 1199. T, V acc: 99.8%, 41.4%. Took: 11.30s\n",
      "Epoch: 1200. T, V acc: 99.8%, 41.3%. Took: 11.27s\n",
      "Epoch: 1201. T, V acc: 99.8%, 40.6%. Took: 11.27s\n",
      "Epoch: 1202. T, V acc: 99.8%, 41.1%. Took: 11.30s\n",
      "Epoch: 1203. T, V acc: 99.8%, 41.5%. Took: 11.33s\n",
      "Epoch: 1204. T, V acc: 99.8%, 41.6%. Took: 11.27s\n",
      "Epoch: 1205. T, V acc: 99.8%, 41.0%. Took: 11.31s\n",
      "Epoch: 1206. T, V acc: 99.8%, 40.8%. Took: 11.33s\n",
      "Epoch: 1207. T, V acc: 99.8%, 41.0%. Took: 11.32s\n",
      "Epoch: 1208. T, V acc: 99.8%, 41.0%. Took: 11.32s\n",
      "Epoch: 1209. T, V acc: 99.8%, 40.4%. Took: 11.32s\n",
      "Epoch: 1210. T, V acc: 99.8%, 40.1%. Took: 11.35s\n",
      "Epoch: 1211. T, V acc: 99.8%, 40.7%. Took: 11.30s\n",
      "Epoch: 1212. T, V acc: 99.8%, 40.7%. Took: 11.29s\n",
      "Epoch: 1213. T, V acc: 99.8%, 40.9%. Took: 11.27s\n",
      "Epoch: 1214. T, V acc: 99.8%, 40.7%. Took: 11.27s\n",
      "Epoch: 1215. T, V acc: 99.8%, 41.4%. Took: 11.24s\n",
      "Epoch: 1216. T, V acc: 99.8%, 41.3%. Took: 11.23s\n",
      "Epoch: 1217. T, V acc: 99.8%, 40.9%. Took: 11.26s\n",
      "Epoch: 1218. T, V acc: 99.8%, 41.0%. Took: 11.22s\n",
      "Epoch: 1219. T, V acc: 99.8%, 41.2%. Took: 11.29s\n",
      "Epoch: 1220. T, V acc: 99.8%, 41.5%. Took: 11.23s\n",
      "Epoch: 1221. T, V acc: 99.8%, 40.9%. Took: 11.23s\n",
      "Epoch: 1222. T, V acc: 99.8%, 40.9%. Took: 11.21s\n",
      "Epoch: 1223. T, V acc: 99.8%, 40.2%. Took: 11.18s\n",
      "Epoch: 1224. T, V acc: 99.8%, 40.3%. Took: 11.24s\n",
      "Epoch: 1225. T, V acc: 99.8%, 40.5%. Took: 11.23s\n",
      "Epoch: 1226. T, V acc: 99.8%, 40.3%. Took: 11.20s\n",
      "Epoch: 1227. T, V acc: 99.8%, 40.4%. Took: 11.23s\n",
      "Epoch: 1228. T, V acc: 99.8%, 40.4%. Took: 11.24s\n",
      "Epoch: 1229. T, V acc: 99.8%, 40.7%. Took: 11.26s\n",
      "Epoch: 1230. T, V acc: 99.8%, 40.8%. Took: 11.25s\n",
      "Epoch: 1231. T, V acc: 99.8%, 40.3%. Took: 11.23s\n",
      "Epoch: 1232. T, V acc: 99.8%, 40.8%. Took: 11.23s\n",
      "Epoch: 1233. T, V acc: 99.8%, 40.7%. Took: 11.24s\n",
      "Epoch: 1234. T, V acc: 99.8%, 40.9%. Took: 11.27s\n",
      "Epoch: 1235. T, V acc: 99.8%, 40.5%. Took: 11.26s\n",
      "Epoch: 1236. T, V acc: 99.8%, 40.5%. Took: 11.28s\n",
      "Epoch: 1237. T, V acc: 99.8%, 40.8%. Took: 11.26s\n",
      "Epoch: 1238. T, V acc: 99.8%, 40.8%. Took: 11.23s\n",
      "Epoch: 1239. T, V acc: 99.9%, 40.7%. Took: 11.20s\n",
      "Epoch: 1240. T, V acc: 99.9%, 40.3%. Took: 11.26s\n",
      "Epoch: 1241. T, V acc: 99.8%, 40.5%. Took: 11.25s\n",
      "Epoch: 1242. T, V acc: 99.8%, 40.2%. Took: 11.26s\n",
      "Epoch: 1243. T, V acc: 99.8%, 40.1%. Took: 11.29s\n",
      "Epoch: 1244. T, V acc: 99.8%, 40.2%. Took: 11.29s\n",
      "Epoch: 1245. T, V acc: 99.8%, 40.6%. Took: 11.27s\n",
      "Epoch: 1246. T, V acc: 99.8%, 40.7%. Took: 11.23s\n",
      "Epoch: 1247. T, V acc: 99.9%, 40.8%. Took: 11.32s\n",
      "Epoch: 1248. T, V acc: 99.8%, 40.7%. Took: 11.33s\n",
      "Epoch: 1249. T, V acc: 99.9%, 40.8%. Took: 11.24s\n",
      "Epoch: 1250. T, V acc: 99.8%, 40.9%. Took: 11.28s\n",
      "Epoch: 1251. T, V acc: 99.8%, 40.5%. Took: 11.30s\n",
      "Epoch: 1252. T, V acc: 99.8%, 40.5%. Took: 11.28s\n",
      "Epoch: 1253. T, V acc: 99.8%, 40.6%. Took: 11.30s\n",
      "Epoch: 1254. T, V acc: 99.9%, 40.7%. Took: 11.31s\n",
      "Epoch: 1255. T, V acc: 99.8%, 40.3%. Took: 11.31s\n",
      "Epoch: 1256. T, V acc: 99.8%, 40.9%. Took: 11.30s\n",
      "Epoch: 1257. T, V acc: 99.8%, 40.0%. Took: 11.30s\n",
      "Epoch: 1258. T, V acc: 99.8%, 40.0%. Took: 11.26s\n",
      "Epoch: 1259. T, V acc: 99.8%, 40.7%. Took: 11.30s\n",
      "Epoch: 1260. T, V acc: 99.8%, 40.9%. Took: 11.29s\n",
      "Epoch: 1261. T, V acc: 99.8%, 40.7%. Took: 11.31s\n",
      "Epoch: 1262. T, V acc: 99.8%, 41.2%. Took: 11.25s\n",
      "Epoch: 1263. T, V acc: 99.8%, 39.9%. Took: 11.27s\n",
      "Epoch: 1264. T, V acc: 99.8%, 39.2%. Took: 11.23s\n",
      "Epoch: 1265. T, V acc: 99.8%, 40.3%. Took: 11.29s\n",
      "Epoch: 1266. T, V acc: 99.8%, 39.8%. Took: 11.27s\n",
      "Epoch: 1267. T, V acc: 99.8%, 39.7%. Took: 11.27s\n",
      "Epoch: 1268. T, V acc: 99.8%, 40.4%. Took: 11.23s\n",
      "Epoch: 1269. T, V acc: 99.8%, 40.6%. Took: 11.27s\n",
      "Epoch: 1270. T, V acc: 99.8%, 40.5%. Took: 11.30s\n",
      "Epoch: 1271. T, V acc: 99.8%, 40.5%. Took: 11.29s\n",
      "Epoch: 1272. T, V acc: 99.8%, 40.4%. Took: 11.27s\n",
      "Epoch: 1273. T, V acc: 99.8%, 40.5%. Took: 11.26s\n",
      "Epoch: 1274. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 1275. T, V acc: 99.8%, 41.0%. Took: 11.28s\n",
      "Epoch: 1276. T, V acc: 99.8%, 40.9%. Took: 11.26s\n",
      "Epoch: 1277. T, V acc: 99.8%, 41.0%. Took: 11.28s\n",
      "Epoch: 1278. T, V acc: 99.8%, 40.8%. Took: 11.23s\n",
      "Epoch: 1279. T, V acc: 99.8%, 40.9%. Took: 11.26s\n",
      "Epoch: 1280. T, V acc: 99.8%, 40.4%. Took: 11.27s\n",
      "Epoch: 1281. T, V acc: 99.8%, 40.6%. Took: 11.30s\n",
      "Epoch: 1282. T, V acc: 99.8%, 40.3%. Took: 11.36s\n",
      "Epoch: 1283. T, V acc: 99.8%, 39.8%. Took: 11.25s\n",
      "Epoch: 1284. T, V acc: 99.8%, 40.0%. Took: 11.21s\n",
      "Epoch: 1285. T, V acc: 99.8%, 40.4%. Took: 11.25s\n",
      "Epoch: 1286. T, V acc: 99.8%, 40.6%. Took: 11.20s\n",
      "Epoch: 1287. T, V acc: 99.8%, 40.7%. Took: 11.25s\n",
      "Epoch: 1288. T, V acc: 99.8%, 40.3%. Took: 11.28s\n",
      "Epoch: 1289. T, V acc: 99.8%, 40.6%. Took: 11.23s\n",
      "Epoch: 1290. T, V acc: 99.8%, 40.6%. Took: 11.30s\n",
      "Epoch: 1291. T, V acc: 99.9%, 40.7%. Took: 11.31s\n",
      "Epoch: 1292. T, V acc: 99.9%, 40.6%. Took: 11.32s\n",
      "Epoch: 1293. T, V acc: 99.8%, 40.6%. Took: 11.33s\n",
      "Epoch: 1294. T, V acc: 99.8%, 40.7%. Took: 11.31s\n",
      "Epoch: 1295. T, V acc: 99.8%, 40.8%. Took: 11.27s\n",
      "Epoch: 1296. T, V acc: 99.8%, 40.9%. Took: 11.25s\n",
      "Epoch: 1297. T, V acc: 99.9%, 40.4%. Took: 11.24s\n",
      "Epoch: 1298. T, V acc: 99.8%, 40.6%. Took: 11.28s\n",
      "Epoch: 1299. T, V acc: 99.8%, 40.6%. Took: 11.27s\n",
      "Epoch: 1300. T, V acc: 99.8%, 40.9%. Took: 11.27s\n",
      "Epoch: 1301. T, V acc: 99.8%, 40.2%. Took: 11.30s\n",
      "Epoch: 1302. T, V acc: 99.8%, 40.4%. Took: 11.27s\n",
      "Epoch: 1303. T, V acc: 99.8%, 40.2%. Took: 11.32s\n",
      "Epoch: 1304. T, V acc: 99.8%, 40.3%. Took: 11.33s\n",
      "Epoch: 1305. T, V acc: 99.8%, 40.6%. Took: 11.30s\n",
      "Epoch: 1306. T, V acc: 99.9%, 40.5%. Took: 11.28s\n",
      "Epoch: 1307. T, V acc: 99.8%, 40.5%. Took: 11.29s\n",
      "Epoch: 1308. T, V acc: 99.8%, 40.5%. Took: 11.25s\n",
      "Epoch: 1309. T, V acc: 99.8%, 40.6%. Took: 11.26s\n",
      "Epoch: 1310. T, V acc: 99.8%, 40.6%. Took: 11.32s\n",
      "Epoch: 1311. T, V acc: 99.8%, 40.4%. Took: 11.26s\n",
      "Epoch: 1312. T, V acc: 99.8%, 40.5%. Took: 11.27s\n",
      "Epoch: 1313. T, V acc: 99.8%, 40.6%. Took: 11.30s\n",
      "Epoch: 1314. T, V acc: 99.8%, 40.7%. Took: 11.26s\n",
      "Epoch: 1315. T, V acc: 99.9%, 40.6%. Took: 11.24s\n",
      "Epoch: 1316. T, V acc: 99.8%, 40.6%. Took: 11.23s\n",
      "Epoch: 1317. T, V acc: 99.9%, 40.6%. Took: 11.24s\n",
      "Epoch: 1318. T, V acc: 99.8%, 40.6%. Took: 11.28s\n",
      "Epoch: 1319. T, V acc: 99.8%, 40.7%. Took: 11.31s\n",
      "Epoch: 1320. T, V acc: 99.9%, 40.8%. Took: 11.32s\n",
      "Epoch: 1321. T, V acc: 99.8%, 40.7%. Took: 11.32s\n",
      "Epoch: 1322. T, V acc: 99.8%, 40.9%. Took: 11.27s\n",
      "Epoch: 1323. T, V acc: 99.8%, 40.7%. Took: 11.27s\n",
      "Epoch: 1324. T, V acc: 99.8%, 40.7%. Took: 11.29s\n",
      "Epoch: 1325. T, V acc: 99.8%, 40.7%. Took: 11.25s\n",
      "Epoch: 1326. T, V acc: 99.8%, 40.8%. Took: 11.26s\n",
      "Epoch: 1327. T, V acc: 99.8%, 40.8%. Took: 11.26s\n",
      "Epoch: 1328. T, V acc: 99.8%, 40.8%. Took: 11.26s\n",
      "Epoch: 1329. T, V acc: 99.8%, 40.7%. Took: 11.25s\n",
      "Epoch: 1330. T, V acc: 99.8%, 40.8%. Took: 11.25s\n",
      "Epoch: 1331. T, V acc: 99.8%, 40.8%. Took: 11.24s\n",
      "Epoch: 1332. T, V acc: 99.8%, 40.6%. Took: 11.29s\n",
      "Epoch: 1333. T, V acc: 99.8%, 40.8%. Took: 11.28s\n",
      "Epoch: 1334. T, V acc: 99.8%, 40.9%. Took: 11.28s\n",
      "Epoch: 1335. T, V acc: 99.8%, 41.1%. Took: 11.30s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1336. T, V acc: 99.8%, 41.0%. Took: 11.32s\n",
      "Epoch: 1337. T, V acc: 99.8%, 41.0%. Took: 11.27s\n",
      "Epoch: 1338. T, V acc: 99.8%, 40.5%. Took: 11.29s\n",
      "Epoch: 1339. T, V acc: 99.8%, 40.3%. Took: 11.27s\n",
      "Epoch: 1340. T, V acc: 99.8%, 40.4%. Took: 11.28s\n",
      "Epoch: 1341. T, V acc: 99.8%, 40.5%. Took: 11.27s\n",
      "Epoch: 1342. T, V acc: 99.8%, 40.6%. Took: 11.28s\n",
      "Epoch: 1343. T, V acc: 99.9%, 40.5%. Took: 11.28s\n",
      "Epoch: 1344. T, V acc: 99.8%, 40.5%. Took: 11.31s\n",
      "Epoch: 1345. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 1346. T, V acc: 99.8%, 40.6%. Took: 11.28s\n",
      "Epoch: 1347. T, V acc: 99.8%, 40.8%. Took: 11.28s\n",
      "Epoch: 1348. T, V acc: 99.8%, 40.8%. Took: 11.32s\n",
      "Epoch: 1349. T, V acc: 99.8%, 40.8%. Took: 11.32s\n",
      "Epoch: 1350. T, V acc: 99.8%, 40.9%. Took: 11.29s\n",
      "Epoch: 1351. T, V acc: 99.8%, 41.0%. Took: 11.31s\n",
      "Epoch: 1352. T, V acc: 99.8%, 40.9%. Took: 11.27s\n",
      "Epoch: 1353. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 1354. T, V acc: 99.9%, 40.8%. Took: 11.31s\n",
      "Epoch: 1355. T, V acc: 99.8%, 40.9%. Took: 11.27s\n",
      "Epoch: 1356. T, V acc: 99.8%, 40.7%. Took: 11.31s\n",
      "Epoch: 1357. T, V acc: 99.8%, 40.8%. Took: 11.28s\n",
      "Epoch: 1358. T, V acc: 99.8%, 40.9%. Took: 11.26s\n",
      "Epoch: 1359. T, V acc: 99.8%, 40.8%. Took: 11.29s\n",
      "Epoch: 1360. T, V acc: 99.8%, 40.7%. Took: 11.24s\n",
      "Epoch: 1361. T, V acc: 99.8%, 40.8%. Took: 11.10s\n",
      "Epoch: 1362. T, V acc: 99.8%, 41.0%. Took: 11.23s\n",
      "Epoch: 1363. T, V acc: 99.8%, 41.0%. Took: 11.27s\n",
      "Epoch: 1364. T, V acc: 99.8%, 41.1%. Took: 11.27s\n",
      "Epoch: 1365. T, V acc: 99.8%, 40.9%. Took: 11.29s\n",
      "Epoch: 1366. T, V acc: 99.8%, 40.8%. Took: 11.28s\n",
      "Epoch: 1367. T, V acc: 99.8%, 40.6%. Took: 11.29s\n",
      "Epoch: 1368. T, V acc: 99.8%, 40.9%. Took: 11.26s\n",
      "Epoch: 1369. T, V acc: 99.8%, 40.7%. Took: 11.19s\n",
      "Epoch: 1370. T, V acc: 99.8%, 40.6%. Took: 11.27s\n",
      "Epoch: 1371. T, V acc: 99.8%, 40.8%. Took: 11.28s\n",
      "Epoch: 1372. T, V acc: 99.8%, 40.7%. Took: 11.31s\n",
      "Epoch: 1373. T, V acc: 99.8%, 40.8%. Took: 11.29s\n",
      "Epoch: 1374. T, V acc: 99.9%, 40.9%. Took: 11.26s\n",
      "Epoch: 1375. T, V acc: 99.8%, 41.2%. Took: 11.27s\n",
      "Epoch: 1376. T, V acc: 99.8%, 41.1%. Took: 11.27s\n",
      "Epoch: 1377. T, V acc: 99.8%, 41.0%. Took: 11.26s\n",
      "Epoch: 1378. T, V acc: 99.8%, 41.2%. Took: 11.29s\n",
      "Epoch: 1379. T, V acc: 99.8%, 41.3%. Took: 11.26s\n",
      "Epoch: 1380. T, V acc: 99.8%, 40.8%. Took: 11.26s\n",
      "Epoch: 1381. T, V acc: 99.8%, 41.1%. Took: 11.27s\n",
      "Epoch: 1382. T, V acc: 99.8%, 40.9%. Took: 11.23s\n",
      "Epoch: 1383. T, V acc: 99.8%, 41.0%. Took: 11.25s\n",
      "Epoch: 1384. T, V acc: 99.8%, 41.1%. Took: 11.25s\n",
      "Epoch: 1385. T, V acc: 99.9%, 40.9%. Took: 11.24s\n",
      "Epoch: 1386. T, V acc: 99.8%, 40.9%. Took: 11.29s\n",
      "Epoch: 1387. T, V acc: 99.8%, 41.1%. Took: 11.27s\n",
      "Epoch: 1388. T, V acc: 99.8%, 41.1%. Took: 11.28s\n",
      "Epoch: 1389. T, V acc: 99.8%, 40.1%. Took: 11.29s\n",
      "Epoch: 1390. T, V acc: 99.8%, 40.4%. Took: 11.24s\n",
      "Epoch: 1391. T, V acc: 99.8%, 40.6%. Took: 11.24s\n",
      "Epoch: 1392. T, V acc: 99.8%, 40.4%. Took: 11.12s\n",
      "Epoch: 1393. T, V acc: 99.8%, 40.7%. Took: 11.21s\n",
      "Epoch: 1394. T, V acc: 99.8%, 40.3%. Took: 11.22s\n",
      "Epoch: 1395. T, V acc: 99.8%, 40.4%. Took: 11.20s\n",
      "Epoch: 1396. T, V acc: 99.8%, 40.6%. Took: 11.22s\n",
      "Epoch: 1397. T, V acc: 99.8%, 40.5%. Took: 11.27s\n",
      "Epoch: 1398. T, V acc: 99.8%, 40.6%. Took: 11.26s\n",
      "Epoch: 1399. T, V acc: 99.9%, 41.2%. Took: 11.27s\n",
      "Epoch: 1400. T, V acc: 99.8%, 40.8%. Took: 11.34s\n",
      "Epoch: 1401. T, V acc: 99.8%, 40.3%. Took: 11.26s\n",
      "Epoch: 1402. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 1403. T, V acc: 99.8%, 40.8%. Took: 11.25s\n",
      "Epoch: 1404. T, V acc: 99.9%, 40.7%. Took: 11.26s\n",
      "Epoch: 1405. T, V acc: 99.8%, 40.9%. Took: 11.26s\n",
      "Epoch: 1406. T, V acc: 99.9%, 40.8%. Took: 11.35s\n",
      "Epoch: 1407. T, V acc: 99.9%, 40.9%. Took: 11.30s\n",
      "Epoch: 1408. T, V acc: 99.9%, 40.9%. Took: 11.28s\n",
      "Epoch: 1409. T, V acc: 99.8%, 40.9%. Took: 11.30s\n",
      "Epoch: 1410. T, V acc: 99.8%, 41.1%. Took: 11.29s\n",
      "Epoch: 1411. T, V acc: 99.9%, 40.9%. Took: 11.32s\n",
      "Epoch: 1412. T, V acc: 99.8%, 40.7%. Took: 11.34s\n",
      "Epoch: 1413. T, V acc: 99.8%, 40.7%. Took: 11.30s\n",
      "Epoch: 1414. T, V acc: 99.8%, 40.8%. Took: 11.32s\n",
      "Epoch: 1415. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 1416. T, V acc: 99.8%, 40.6%. Took: 11.37s\n",
      "Epoch: 1417. T, V acc: 99.8%, 40.6%. Took: 11.26s\n",
      "Epoch: 1418. T, V acc: 99.8%, 40.3%. Took: 11.27s\n",
      "Epoch: 1419. T, V acc: 99.8%, 40.3%. Took: 11.28s\n",
      "Epoch: 1420. T, V acc: 99.8%, 40.4%. Took: 11.27s\n",
      "Epoch: 1421. T, V acc: 99.8%, 40.3%. Took: 11.29s\n",
      "Epoch: 1422. T, V acc: 99.8%, 40.6%. Took: 11.32s\n",
      "Epoch: 1423. T, V acc: 99.8%, 40.6%. Took: 11.34s\n",
      "Epoch: 1424. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 1425. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 1426. T, V acc: 99.8%, 40.6%. Took: 11.25s\n",
      "Epoch: 1427. T, V acc: 99.8%, 40.3%. Took: 11.25s\n",
      "Epoch: 1428. T, V acc: 99.8%, 39.8%. Took: 11.26s\n",
      "Epoch: 1429. T, V acc: 99.8%, 40.0%. Took: 11.28s\n",
      "Epoch: 1430. T, V acc: 99.8%, 40.2%. Took: 11.26s\n",
      "Epoch: 1431. T, V acc: 99.8%, 40.2%. Took: 11.26s\n",
      "Epoch: 1432. T, V acc: 99.8%, 39.8%. Took: 11.24s\n",
      "Epoch: 1433. T, V acc: 99.8%, 39.9%. Took: 11.28s\n",
      "Epoch: 1434. T, V acc: 99.8%, 40.2%. Took: 11.24s\n",
      "Epoch: 1435. T, V acc: 99.8%, 40.5%. Took: 11.27s\n",
      "Epoch: 1436. T, V acc: 99.8%, 40.3%. Took: 11.27s\n",
      "Epoch: 1437. T, V acc: 99.9%, 40.4%. Took: 11.26s\n",
      "Epoch: 1438. T, V acc: 99.8%, 40.7%. Took: 11.29s\n",
      "Epoch: 1439. T, V acc: 99.9%, 40.7%. Took: 11.32s\n",
      "Epoch: 1440. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 1441. T, V acc: 99.8%, 40.8%. Took: 11.30s\n",
      "Epoch: 1442. T, V acc: 99.8%, 40.9%. Took: 11.33s\n",
      "Epoch: 1443. T, V acc: 99.9%, 40.9%. Took: 11.29s\n",
      "Epoch: 1444. T, V acc: 99.8%, 40.9%. Took: 11.29s\n",
      "Epoch: 1445. T, V acc: 99.9%, 40.7%. Took: 11.28s\n",
      "Epoch: 1446. T, V acc: 99.8%, 40.5%. Took: 11.29s\n",
      "Epoch: 1447. T, V acc: 99.8%, 40.5%. Took: 11.28s\n",
      "Epoch: 1448. T, V acc: 99.8%, 40.5%. Took: 11.28s\n",
      "Epoch: 1449. T, V acc: 99.8%, 40.9%. Took: 11.24s\n",
      "Epoch: 1450. T, V acc: 99.8%, 40.8%. Took: 11.24s\n",
      "Epoch: 1451. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 1452. T, V acc: 99.8%, 40.8%. Took: 11.31s\n",
      "Epoch: 1453. T, V acc: 99.8%, 40.7%. Took: 11.27s\n",
      "Epoch: 1454. T, V acc: 99.8%, 41.0%. Took: 11.26s\n",
      "Epoch: 1455. T, V acc: 99.8%, 40.8%. Took: 11.26s\n",
      "Epoch: 1456. T, V acc: 99.8%, 40.6%. Took: 11.24s\n",
      "Epoch: 1457. T, V acc: 99.8%, 40.3%. Took: 11.28s\n",
      "Epoch: 1458. T, V acc: 99.8%, 40.5%. Took: 11.31s\n",
      "Epoch: 1459. T, V acc: 99.8%, 40.3%. Took: 11.28s\n",
      "Epoch: 1460. T, V acc: 99.8%, 40.8%. Took: 11.22s\n",
      "Epoch: 1461. T, V acc: 99.8%, 40.6%. Took: 11.30s\n",
      "Epoch: 1462. T, V acc: 99.8%, 40.5%. Took: 11.30s\n",
      "Epoch: 1463. T, V acc: 99.8%, 40.6%. Took: 11.30s\n",
      "Epoch: 1464. T, V acc: 99.8%, 40.7%. Took: 11.27s\n",
      "Epoch: 1465. T, V acc: 99.8%, 40.8%. Took: 11.27s\n",
      "Epoch: 1466. T, V acc: 99.8%, 40.7%. Took: 11.28s\n",
      "Epoch: 1467. T, V acc: 99.8%, 39.8%. Took: 11.28s\n",
      "Epoch: 1468. T, V acc: 99.8%, 40.1%. Took: 11.26s\n",
      "Epoch: 1469. T, V acc: 99.8%, 40.1%. Took: 11.27s\n",
      "Epoch: 1470. T, V acc: 99.8%, 40.4%. Took: 11.27s\n",
      "Epoch: 1471. T, V acc: 99.8%, 40.4%. Took: 11.17s\n",
      "Epoch: 1472. T, V acc: 99.8%, 40.6%. Took: 11.33s\n",
      "Epoch: 1473. T, V acc: 99.8%, 40.5%. Took: 11.23s\n",
      "Epoch: 1474. T, V acc: 99.8%, 40.8%. Took: 11.32s\n",
      "Epoch: 1475. T, V acc: 99.9%, 40.7%. Took: 11.30s\n",
      "Epoch: 1476. T, V acc: 99.9%, 40.7%. Took: 11.29s\n",
      "Epoch: 1477. T, V acc: 99.9%, 40.6%. Took: 11.31s\n",
      "Epoch: 1478. T, V acc: 99.9%, 40.8%. Took: 11.30s\n",
      "Epoch: 1479. T, V acc: 99.8%, 40.2%. Took: 11.29s\n",
      "Epoch: 1480. T, V acc: 99.8%, 40.7%. Took: 11.26s\n",
      "Epoch: 1481. T, V acc: 99.8%, 40.5%. Took: 11.26s\n",
      "Epoch: 1482. T, V acc: 99.9%, 40.9%. Took: 11.26s\n",
      "Epoch: 1483. T, V acc: 99.8%, 40.8%. Took: 11.31s\n",
      "Epoch: 1484. T, V acc: 99.9%, 40.8%. Took: 11.31s\n",
      "Epoch: 1485. T, V acc: 99.8%, 41.0%. Took: 11.33s\n",
      "Epoch: 1486. T, V acc: 99.8%, 40.7%. Took: 11.30s\n",
      "Epoch: 1487. T, V acc: 99.8%, 40.9%. Took: 11.22s\n",
      "Epoch: 1488. T, V acc: 99.8%, 41.0%. Took: 11.26s\n",
      "Epoch: 1489. T, V acc: 99.8%, 41.0%. Took: 11.29s\n",
      "Epoch: 1490. T, V acc: 99.8%, 41.1%. Took: 11.24s\n",
      "Epoch: 1491. T, V acc: 99.9%, 40.9%. Took: 11.27s\n",
      "Epoch: 1492. T, V acc: 99.8%, 40.7%. Took: 11.31s\n",
      "Epoch: 1493. T, V acc: 99.8%, 40.7%. Took: 11.24s\n",
      "Epoch: 1494. T, V acc: 99.8%, 40.9%. Took: 11.35s\n",
      "Epoch: 1495. T, V acc: 99.8%, 40.5%. Took: 11.37s\n",
      "Epoch: 1496. T, V acc: 99.9%, 40.7%. Took: 11.32s\n",
      "Epoch: 1497. T, V acc: 99.9%, 40.8%. Took: 11.29s\n",
      "Epoch: 1498. T, V acc: 99.9%, 40.8%. Took: 11.32s\n",
      "Epoch: 1499. T, V acc: 99.8%, 40.9%. Took: 11.33s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1500. T, V acc: 99.9%, 41.0%. Took: 11.32s\n",
      "Epoch: 1501. T, V acc: 99.8%, 40.9%. Took: 11.33s\n",
      "Epoch: 1502. T, V acc: 99.8%, 40.9%. Took: 11.72s\n",
      "Epoch: 1503. T, V acc: 99.8%, 41.0%. Took: 11.32s\n",
      "Epoch: 1504. T, V acc: 99.8%, 40.4%. Took: 11.66s\n",
      "Epoch: 1505. T, V acc: 99.9%, 40.5%. Took: 11.44s\n",
      "Epoch: 1506. T, V acc: 99.8%, 41.1%. Took: 11.39s\n",
      "Epoch: 1507. T, V acc: 99.8%, 41.2%. Took: 11.43s\n",
      "Epoch: 1508. T, V acc: 99.9%, 41.2%. Took: 11.48s\n",
      "Epoch: 1509. T, V acc: 99.8%, 41.2%. Took: 11.47s\n",
      "Epoch: 1510. T, V acc: 99.9%, 41.2%. Took: 11.27s\n",
      "Epoch: 1511. T, V acc: 99.8%, 41.4%. Took: 11.58s\n",
      "Epoch: 1512. T, V acc: 99.8%, 41.4%. Took: 11.55s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-0437d44f0bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_set\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_sets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"info_{train_set.name}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-52423cc20e9a>\u001b[0m in \u001b[0;36mdo_training\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m                            \u001b[0mfun_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                            \u001b[0mfun_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                            n_epochs=self.n_epochs)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-d9270e143105>\u001b[0m in \u001b[0;36mdo_training\u001b[0;34m(model, name, iter_train, iter_valid, optimizer, criterion, fun_train, fun_eval, n_epochs, train_info)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mclock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melapsed_since_start\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# meh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-3a946dd401f9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sources/nn-training/venv/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sources/nn-training/venv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for train_set in train_sets:\n",
    "    train_set.init()\n",
    "    train_info = train_set.do_training()\n",
    "    \n",
    "    train_info.save(f\"info_{train_set.name}.pickle\")\n",
    "    \n",
    "    # plot loss data\n",
    "    fig, ax = plt.subplots(figsize=(20, 8))\n",
    "    ax_train = ax.plot(list(range(train_set.n_epochs)), train_info.train[\"loss\"], label=\"train\")\n",
    "    ax_valid = ax.plot(list(range(train_set.n_epochs)), train_info.valid[\"loss\"], label=\"valid\")\n",
    "    fig.legend()\n",
    "    fig.suptitle(\"Validation loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    # save\n",
    "    save_vocab_embedding(\"vocab_emb\", TEXT.vocab, model.embedding.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peeking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tokens(tokens, model, device=DEVICE):\n",
    "    model.eval()\n",
    "    idxs = [TEXT.vocab.stoi[t] for t in tokens]\n",
    "    inp = torch.LongTensor(idxs).reshape(-1, 1).to(device)\n",
    "    output = output_to_pred(model((inp, torch.LongTensor([len(tokens)]))))\n",
    "    return output.item()\n",
    "\n",
    "\n",
    "def predict(sentence, model):\n",
    "    return predict_tokens(list(map(str, nlp.tokenizer(sentence))), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = default_model(hidden_dim=256, n_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('rnn_hidden-256_nlayers-3_nepochs-10000.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.0, 4.0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"Du temps perdu.\", model), \\\n",
    "predict(\"Un trs bon film,  voir avec toute la famille.\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un peu  l' image de sa filmographie , Roger Michell semble tiraill entre un dsir sincre de donner corps aux asprits de son drame humain et ses habitudes de faiseur disciplin .\n",
      "true) 3.0 - 3.0 (pred\n",
      "Une raret : le reggae cont par ceux qui le font . A chacun son histoire , parfois dure , toujours touchante .\n",
      "true) 4.0 - 4.0 (pred\n",
      "Ici , le morceau de sparadrap dont ,  l image du capitaine Haddock , Elia Suleiman ne parvient pas  se dfaire , c est son propre pays , la Palestine . Il en rsulte une absurde comdie de l absurde , d une magnifique et tendre mlancolie . Indispensable .\n",
      "true) 4.0 - 3.5 (pred\n"
     ]
    }
   ],
   "source": [
    "for example in random.sample(list(data_train), 3):\n",
    "    tokens, note = example.input, float(example.target)\n",
    "    print(\" \".join(tokens))\n",
    "    print(f\"true) {note * 5} - {predict_tokens(tokens, model)} (pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looser accuracies for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the model really doing bad if it predicts a 4.5 instead of a 5 ? There are at least two ways to allow for forgivable divergence with the test data :\n",
    "* decrease notation's granularity, e.g. tranform the marks into good/bad, or good/bad/neutral.\n",
    "* consider a prediction correct if it belongs to a 'small' interval containing the true value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good/Neutral/Bad prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_to_3_way(pred_tensor, bad_treshold=.375, good_treshold=.625):\n",
    "    \"\"\"np array with values: 0: bad, 1: neutral, 2: good\"\"\"\n",
    "    return np.digitize(pred_tensor.cpu().detach().numpy(), [bad_treshold, good_treshold])\n",
    "    \n",
    "def eval_accuracy_3w(model, iterator):\n",
    "    n_examples = 0\n",
    "    n_success = 0\n",
    "    for batch in iterator:\n",
    "        predictions = model(batch.input).squeeze(1)\n",
    "        n_examples += len(batch)\n",
    "        n_success += sum(normalized_to_3_way(predictions) == normalized_to_3_way(batch.target))\n",
    "    return n_success / n_examples\n",
    "\n",
    "def classif_report_3w(model, iterator):\n",
    "    def preds_and_trues_to_array(predictions, true_targets):\n",
    "        return np.concatenate([normalized_to_3_way(true_targets).reshape(-1, 1),\n",
    "                               normalized_to_3_way(predictions).reshape(-1, 1)], axis=1)\n",
    "    \n",
    "    array = None\n",
    "    for batch in iterator:\n",
    "        predictions = model(batch.input).squeeze(1)\n",
    "        \n",
    "        if array is None:\n",
    "            array = preds_and_trues_to_array(predictions, batch.target)\n",
    "        else:\n",
    "            array = np.concatenate([array,\n",
    "                                    preds_and_trues_to_array(predictions, batch.target)], axis=0)\n",
    "    print(sklearn.metrics.classification_report(array[:, 0],\n",
    "                                                array[:, 1],\n",
    "                                                labels=[0, 1, 2],\n",
    "                                                target_names=[\"bad\", \"neutral\", \"good\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.36      0.22      0.28      1661\n",
      "     neutral       0.65      0.60      0.63      9672\n",
      "        good       0.72      0.82      0.77     10743\n",
      "\n",
      "    accuracy                           0.68     22076\n",
      "   macro avg       0.58      0.55      0.56     22076\n",
      "weighted avg       0.67      0.68      0.67     22076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iterator = iter_test\n",
    "\n",
    "classif_report_3w(model, iterator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy_fuzzy(model, iterator, fuzziness=.1):\n",
    "    n_examples = 0.\n",
    "    n_success = 0.\n",
    "    for batch in iterator:\n",
    "        predictions = model(batch.input).squeeze(1)\n",
    "        n_examples += len(batch)\n",
    "        n_success += sum(torch.abs(predictions - batch.target) <= fuzziness).item()\n",
    "    return n_success / n_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7549827867367277"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_accuracy_fuzzy(model, iter_test, fuzziness=.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
